{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86d95d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../reconstruct_missing_data')\n",
    "\n",
    "from data_loading import find_data_files, load_data_set, get_anomalies, clone_data, create_missing_mask, split_and_scale_data\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "from json import dump, load\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7a95b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gxfs_work1/geomar/smomw511/GitHub/MarcoLandtHayen/reconstruct_missing_data/notebooks\n"
     ]
    }
   ],
   "source": [
    "# Check current working directory:\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a0dc335",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set parameters up-front:\n",
    "\n",
    "## Decide to work on test data or full data:\n",
    "path_to_data = '../data/test_data/' # Test data\n",
    "# path_to_data = '../../../../climate_index_collection/data/raw/2022-08-22/' # Full data\n",
    "\n",
    "# Model configuration, to store results:\n",
    "model_config = 'unet_4conv'\n",
    "\n",
    "# Data loading and preprocessing:\n",
    "feature = 'sea-level-pressure' # Choose either 'sea-level-pressure' or 'sea-surface-temperature' as feature.\n",
    "feature_short = 'slp' # Free to set short name, to store results, e.g. 'slp' and 'sst'.\n",
    "source = 'FOCI' # Choose Earth System Model, either 'FOCI' or 'CESM'.\n",
    "mask_type = 'fixed' # Can have random missing values, individually for each data sample ('variable'), \n",
    "                    # or randomly create only a single mask, that is then applied to all samples identically ('fixed').\n",
    "missing_type = 'discrete' # Either specify discrete amounts of missing values ('discrete') or give a range ('range').\n",
    "augmentation_factor = 1 # Number of times, each sample is to be cloned, keeping the original order.\n",
    "train_val_split = 0.8 # Set rel. amount of samples used for training.\n",
    "missing_values = [0.99, 0.95, 0.9, 0.75, 0.5] # Set array for desired amounts of missing values: 0.9 means, that 90% of the values are missing.\n",
    "                                              # Or set a range by only giving minimum and maximum allowed relative amounts of missing values, \n",
    "                                              # e.g. [0.75, 0.95], according to missing_type 'discrete' or 'range', respectively.\n",
    "scale_to = 'zero_one' # Choose to scale inputs to [-1,1] ('one_one') or [0,1] ('zero_one') or 'norm' to normalize inputs or 'no' scaling.\n",
    "\n",
    "# To build, compile and train model:\n",
    "CNN_filters = [64,128,256,512]# [2,4,8,16] # Number of filters.\n",
    "CNN_kernel_size = 5 # Kernel size\n",
    "learning_rate = 0.0005\n",
    "loss_function = 'mse' \n",
    "epochs = 10\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32f6212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to store results: Raise error, if path already exists, to avoid overwriting existing results. \n",
    "path = Path('../../../../GitGeomar/marco-landt-hayen/reconstruct_missing_data/results/'+model_config+'_'+feature_short+'_'+source+'_'+mask_type+'_'+missing_type+'_factor_'+str(augmentation_factor))\n",
    "os.makedirs(path, exist_ok=False)\n",
    "    \n",
    "# Store parameters as json:\n",
    "parameters = {\n",
    "    'model_config': model_config,\n",
    "    'feature': feature,\n",
    "    'feature_short': feature_short,\n",
    "    'source': source,\n",
    "    'mask_type': mask_type,\n",
    "    'missing_type': missing_type,\n",
    "    'augmentation_factor': augmentation_factor,\n",
    "    'train_val_split': train_val_split,\n",
    "    'missing_values': missing_values,\n",
    "    'scale_to': scale_to,\n",
    "    'CNN_filters': CNN_filters,\n",
    "    'CNN_kernel_size': CNN_kernel_size,\n",
    "    'learning_rate': learning_rate,\n",
    "    'loss_function': loss_function,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size\n",
    "}\n",
    "\n",
    "with open(path / 'parameters.json', 'w') as f:\n",
    "    dump(parameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3eb9921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../data/test_data/FOCI/FOCI1.3-SW038_1m_23500101_23591231_grid_T_atmos_grid.nc'),\n",
       " PosixPath('../data/test_data/FOCI/FOCI1.3-SW038_echam6_ATM_mm_2350-2359_geopoth_pl_monthly_50000.nc'),\n",
       " PosixPath('../data/test_data/FOCI/FOCI1.3-SW038_echam6_BOT_mm_2350-2359_precip_monthly_1.nc'),\n",
       " PosixPath('../data/test_data/FOCI/FOCI1.3-SW038_echam6_BOT_mm_2350-2359_slp_monthly_1.nc'),\n",
       " PosixPath('../data/test_data/FOCI/FOCI1.3-SW038_echam6_BOT_mm_2350-2359_temp2_monthly_1.nc'),\n",
       " PosixPath('../data/test_data/FOCI/FOCI1.3-SW038_echam6_BOT_mm_2350-2359_tsw_monthly_1.nc')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look for data files:\n",
    "find_data_files(data_path=path_to_data, data_source_name=source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20936b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/conda/lib/python3.8/site-packages/xarray/coding/times.py:673: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  dtype = _decode_cf_datetime_dtype(data, units, calendar, self.use_cftime)\n",
      "/app/conda/lib/python3.8/site-packages/numpy/core/_asarray.py:83: SerializationWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# Load data, including ALL fields and mask for Ocean values:\n",
    "data = load_data_set(data_path=path_to_data, data_source_name=source)\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41cf3d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select single feature and compute anomalies, using whole time span as climatology:\n",
    "data = get_anomalies(feature=feature, data_set=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5302f098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend data, if desired:\n",
    "data_temp = clone_data(data=data, augmentation_factor=augmentation_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99eae1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_min=missing_values[0]\n",
    "missing_max=missing_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9621bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mask for missing values:\n",
    "missing_mask = create_missing_mask(data=data, mask_type=mask_type, missing_type=missing_type, missing_min=missing_min, missing_max=missing_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4791b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183,\n",
       "       183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183,\n",
       "       183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183,\n",
       "       183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183,\n",
       "       183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183,\n",
       "       183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183,\n",
       "       183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183,\n",
       "       183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183,\n",
       "       183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183, 183,\n",
       "       183, 183, 183])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(missing_mask,axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "119e3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sparse data as inputs and complete data as targets. Split sparse and complete data into training and validation sets. \n",
    "# Scale or normlalize data according to statistics obtained from only training data.\n",
    "train_input, val_input, train_target, val_target, train_min, train_max, train_mean, train_std = split_and_scale_data(\n",
    "    data, \n",
    "    missing_mask, \n",
    "    train_val_split, \n",
    "    scale_to\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c00f02a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train models:\n",
    "\n",
    "# Loop over array of desired sparsity:\n",
    "for i in range(len(sparsity_all)):\n",
    "    \n",
    "    # Get current sparsity:\n",
    "    sparsity = sparsity_all[i]\n",
    "    \n",
    "    # Print status:\n",
    "    print(\"Sparsity: \", i+1, \" of \", len(sparsity_all))\n",
    "    \n",
    "    # Create sub-directory to store results: Raise error, if path already exists, to avoid overwriting existing results.\n",
    "    os.makedirs(path / 'sparsity_' f'{int(sparsity*100)}', exist_ok=False)\n",
    "    \n",
    "    # Load data:\n",
    "    data = load_data(source=source)\n",
    "    \n",
    "    # Extend data, if desired:\n",
    "    data = clone_data(data=data, augmentation_factor=augmentation_factor)\n",
    "    \n",
    "    # Create sparsity mask:\n",
    "    sparsity_mask = create_sparsity_mask(data=data, sparsity=sparsity, mask_type=mask_type)\n",
    "    \n",
    "    # Store sparsity mask:\n",
    "    np.save(path / 'sparsity_' f'{int(sparsity*100)}' / 'sparsity_mask.npy', sparsity_mask)\n",
    "\n",
    "    # Use sparse data as inputs and complete data as targets. Split sparse and complete data into training and validation sets. \n",
    "    # Scale or normlalize data according to statistics obtained from only training data.\n",
    "    train_input, val_input, train_target, val_target, train_min, train_max, train_mean, train_std = split_and_scale_data(\n",
    "        data, \n",
    "        missing_mask, \n",
    "        train_val_split, \n",
    "        scale_to\n",
    "    )\n",
    "    \n",
    "    # Build and compile U-Net model:\n",
    "    model = build_unet_4conv(input_shape=(train_input.shape[1],train_input.shape[2],1),\n",
    "                         CNN_filters=CNN_filters,\n",
    "                         CNN_kernel_size=CNN_kernel_size,\n",
    "                         learning_rate=learning_rate,\n",
    "                         loss_function=loss_function,\n",
    "                        )\n",
    "    \n",
    "    # Save untrained model:\n",
    "    model.save(path / 'sparsity_' f'{int(sparsity*100)}' / f'epoch_{0}')\n",
    "    \n",
    "    # Initialize storage for training and validation loss:\n",
    "    train_loss=[]\n",
    "    val_loss=[]\n",
    "    \n",
    "    # Get model predictions on train and validation data FROM UNTRAINED MODEL!\n",
    "    train_pred = model.predict(train_input)\n",
    "    val_pred = model.predict(val_input)\n",
    "    \n",
    "    # Store loss on training and validation data:\n",
    "    train_loss.append(np.mean((train_pred[:,:,:,0]-train_target)**2))\n",
    "    val_loss.append(np.mean((val_pred[:,:,:,0]-val_target)**2))\n",
    "    \n",
    "    # Loop over number of training epochs:\n",
    "    for j in range(epochs):\n",
    "        \n",
    "        # Print status:\n",
    "        print(\"  Epoch: \", j+1, \" of \", epochs)\n",
    "        \n",
    "        # Train model on sparse inputs with complete 2D fields as targets, for SINGLE epoch:\n",
    "        history = model.fit(train_input, train_target, epochs=1, verbose=0, shuffle=True,\n",
    "                            batch_size=batch_size, validation_data=(val_input, val_target))\n",
    "        \n",
    "        # Save trained model after current epoch:\n",
    "        model.save(path / 'sparsity_' f'{int(sparsity*100)}' / f'epoch_{j+1}')\n",
    "        \n",
    "        # Get model predictions on train and validation data AFTER current epoch:\n",
    "        train_pred = model.predict(train_input)\n",
    "        val_pred = model.predict(val_input)\n",
    "\n",
    "        # Store loss on training and validation data:\n",
    "        train_loss.append(np.mean((train_pred[:,:,:,0]-train_target)**2))\n",
    "        val_loss.append(np.mean((val_pred[:,:,:,0]-val_target)**2))  \n",
    "        \n",
    "    # Save loss:\n",
    "    np.save(path / 'sparsity_' f'{int(sparsity*100)}' / 'train_loss.npy', train_loss)\n",
    "    np.save(path / 'sparsity_' f'{int(sparsity*100)}' / 'val_loss.npy', val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84bc91a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0530aad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6d335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8efe38fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 96, 192)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract single field, here: Sea level pressure\n",
    "slp_FOCI = data_FOCI['sea-level-pressure'].values\n",
    "slp_FOCI.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

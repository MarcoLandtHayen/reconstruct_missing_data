{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ab89d6",
   "metadata": {},
   "source": [
    "### First experiments with U-Net on real world sst data\n",
    "\n",
    "Try U-Net following [Xiantao et al., 2020](https://www.nature.com/articles/s41598-020-59801-x), but with only 4 convolutions, opposed to 5.\n",
    "Work with real world slp samples. Omit final row, to have number of rows evenly divideable by 2 for all three max pooling operations.\n",
    "\n",
    "Choose **mask type**: Can either have random sparsity mask, individually for each data sample ('variable'), or create only a single random mask, that is then applied to all samples identically ('fixed'). \n",
    "And optionally extend data with **augmentation factor**: Specify the number of times, each sample is to be cloned, keeping the original order.\n",
    "\n",
    "#### Experiments:\n",
    "\n",
    "1) **mask_type='fixed', augmentation_factor=1:** Base experiment, according to szenario to have limitet number of measurements from stations, that are fixed in their location. Fast to train, low ressources required.\n",
    "2) **mask_type='variable', augmentation_factor=1:** Extends base experiment, can handle variable inputs with fixed sparsity in single model. Fits to observations from e.g. argo floats on their trajectory or well suited for e.g. sea surface temperature measurements, where varying cloud coverage limits infrared observations. In its simples form, use each sample only once. Expect worse performance, commpared to base experiment.\n",
    "3) **mask_type='variable', augmentation_factor>0:** Improve performance by using each input sample multiple times, allowed by random sparsity. Try to reduce overfitting and increase generalization on unseen data. Comes at the expense of increased training time.\n",
    "4) **mask_type='optimal', augmentation_factor=1:** Use optimal mask from relevance experiment with range model.\n",
    "\n",
    "Use **sparse and scaled (to [0,1]) samples as inputs**. Use **complete and scaled (to [0,1]) samples as targets**.\n",
    "\n",
    "Got results on nesh, using GPU cluster node with batch **unet_4conv_slp_realworld.py**. Specify **mask_type** and **augmentation_factor**!\n",
    "\n",
    "#### Results:\n",
    "\n",
    "In general, get some nice results: Model predictions capture the main structure in slp anomalies, even for *sparsity=0.99*.\n",
    "\n",
    "In terms of mean squarred error on training and validation data, models from our base experiment (experiment 1) trained on **fixed** sparsity mask **outperformes** models trained on **variable** mask, at least **without data augmentation** (experiment 2), as expected. But it's a trade off, since models from experiment 2 have higher flexibility: Can feed samples with specified sparsity but can freely choose, *which* inputs to use. The price is - as mentioned - a slightly worse reconstruction quality, which can be hardly recognized with human eye.\n",
    "\n",
    "Adding data augmentation **factor=2** (experiment 3), model performance significantly improves and beats performance from our base experiment, keeping full flexibility in a sense, that we can freely choose input pixels, at least if we keep sparsity fixed.\n",
    "\n",
    "**Note:** Need multi-run experiments for each setup, show error bars. And run experiment 3 with higher augmentation factors to validate the story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd5707b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../reconstruct_missing_data')\n",
    "\n",
    "from pathlib import Path\n",
    "from json import dump, load\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from data_loading import find_data_files, load_data_set, get_anomalies, clone_data, create_missing_mask, split_and_scale_data\n",
    "from models import build_unet_4conv\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, concatenate, Conv1D, Conv2D, MaxPool2D, UpSampling2D, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow.keras.initializers as tfi\n",
    "import tensorflow.keras.regularizers as tfr\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Suppress Tensorflow warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f265be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory, according to working directory in scripts:\n",
    "os.chdir('/gxfs_work1/geomar/smomw511')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b0c1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_sst_realworld_fixed_discrete_factor_1_seed_1\n"
     ]
    }
   ],
   "source": [
    "## Specify base parameters for experiments to compare:\n",
    "\n",
    "# exp1: fixed mask\n",
    "# exp2: variable mask, augmentation factor 1\n",
    "# exp3a: variable mask, augmentation factor 2\n",
    "# exp3b: variable mask, augmentation factor 3\n",
    "# exp4: optimal mask\n",
    "\n",
    "# Common parameters for all experiments:\n",
    "model_config = 'unet_4conv'\n",
    "feature = 'sea-surface-temperature' # Choose either 'sea-level-pressure' or 'sea-surface-temperature' as feature.\n",
    "feature_short = 'sst' # Free to set short name, to store results, e.g. 'slp' and 'sst'.\n",
    "source = 'realworld' # Choose Earth System Model, either 'FOCI' or 'CESM', or 'realworld'.\n",
    "\n",
    "# Specific parameters for exp1:\n",
    "mask_types = ['fixed']#,'fixed','fixed']\n",
    "missing_types = ['discrete']#,'discrete','discrete']\n",
    "augmentation_factors = [1]#,1,1]\n",
    "runs = ['_seed_1']#, '_seed_2', '_seed_3']\n",
    "\n",
    "# Reconstruct path to experiment:\n",
    "paths = []\n",
    "for i in range(len(mask_types)):\n",
    "    paths.append(Path('GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/'+model_config+'_'+feature_short+'_'+source+'_'\n",
    "                      +mask_types[i]+'_'+missing_types[i]+'_factor_'+str(augmentation_factors[i])+runs[i]))\n",
    "    print(paths[i])\n",
    "\n",
    "# Initialize storage for parameters of all experiments:\n",
    "parameters_all = []\n",
    "\n",
    "# Reload parameters for all experiments:\n",
    "for path in paths:\n",
    "    \n",
    "    # Open corresponding parameter file and store:\n",
    "    with open(path / 'parameters.json', 'r') as f:\n",
    "        parameters_all.append(load(f))\n",
    "\n",
    "# Convert to numpy array:\n",
    "parameters_all = np.array(parameters_all)\n",
    "\n",
    "# Extract certain parameters from first experiment. Note: These parameters should be identical for all experiments of interest, here.\n",
    "train_val_split = parameters_all[0]['train_val_split']\n",
    "missing_values = parameters_all[0]['missing_values']\n",
    "scale_to = parameters_all[0]['scale_to']\n",
    "epochs = parameters_all[0]['epochs']\n",
    "\n",
    "## Get an overview of final train and validation loss, depending on rel. amount of missing values, for all experiments.\n",
    "## Note: Start with epoch 0, hence untrained model.\n",
    "\n",
    "# Initialize storage for final train and validation loss. Dimension: (#experiments, #missing values settings, #epochs+1)\n",
    "train_loss_final_all = np.zeros((len(paths),len(missing_values),epochs+1))\n",
    "val_loss_final_all = np.zeros((len(paths),len(missing_values),epochs+1))\n",
    "\n",
    "# Loop over experiments:\n",
    "for i in range(len(paths)):\n",
    "    \n",
    "    # Get path for current experiment:\n",
    "    path = paths[i]\n",
    "\n",
    "    # Loop over array of desired rel. amounts of missing values:\n",
    "    for j in range(len(missing_values)):\n",
    "\n",
    "        # Get current sparsity:\n",
    "        missing = missing_values[j]\n",
    "\n",
    "        # Reload loss.\n",
    "        # Rel. amount of missing values = 0.999 requires special treatment:\n",
    "        if missing==0.999:\n",
    "            train_loss = np.load(path / 'missing_' f'{int(missing*1000)}'/ 'train_loss.npy')\n",
    "            val_loss = np.load(path / 'missing_' f'{int(missing*1000)}'/ 'val_loss.npy')\n",
    "        else:\n",
    "            train_loss = np.load(path / 'missing_' f'{int(missing*100)}'/ 'train_loss.npy')\n",
    "            val_loss = np.load(path / 'missing_' f'{int(missing*100)}'/ 'val_loss.npy')\n",
    "            \n",
    "        # Store final train and validation loss:\n",
    "        train_loss_final_all[i,j,:] = train_loss\n",
    "        val_loss_final_all[i,j,:] = val_loss\n",
    "        \n",
    "# Get minimum train and validation loss over epochs for experiment 1:\n",
    "train_loss_min_exp1 = np.min(train_loss_final_all,axis=-1)\n",
    "val_loss_min_exp1 = np.min(val_loss_final_all,axis=-1)\n",
    "\n",
    "# Get mean and std dev of minimum train and validation loss over all runs:\n",
    "train_loss_min_mean_exp1 = np.mean(train_loss_min_exp1,axis=0)\n",
    "val_loss_min_mean_exp1 = np.mean(val_loss_min_exp1,axis=0)\n",
    "train_loss_min_std_exp1 = np.std(train_loss_min_exp1,axis=0)\n",
    "val_loss_min_std_exp1 = np.std(val_loss_min_exp1,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e9d49e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp -r unet_4conv_sst_realworld_fixed_discrete_factor_1_seed_1/missing_999/epoch_10 unet_4conv_sst_realworld_fixed_discrete_factor_1_final/missing_999/model\n",
      "cp -r unet_4conv_sst_realworld_fixed_discrete_factor_1_seed_1/missing_999/missing_mask.npy unet_4conv_sst_realworld_fixed_discrete_factor_1_final/missing_999\n",
      "cp -r unet_4conv_sst_realworld_fixed_discrete_factor_1_seed_1/missing_99/epoch_10 unet_4conv_sst_realworld_fixed_discrete_factor_1_final/missing_99/model\n",
      "cp -r unet_4conv_sst_realworld_fixed_discrete_factor_1_seed_1/missing_99/missing_mask.npy unet_4conv_sst_realworld_fixed_discrete_factor_1_final/missing_99\n",
      "cp -r unet_4conv_sst_realworld_fixed_discrete_factor_1_seed_1/missing_95/epoch_10 unet_4conv_sst_realworld_fixed_discrete_factor_1_final/missing_95/model\n",
      "cp -r unet_4conv_sst_realworld_fixed_discrete_factor_1_seed_1/missing_95/missing_mask.npy unet_4conv_sst_realworld_fixed_discrete_factor_1_final/missing_95\n",
      "cp -r unet_4conv_sst_realworld_fixed_discrete_factor_1_seed_1/missing_90/epoch_10 unet_4conv_sst_realworld_fixed_discrete_factor_1_final/missing_90/model\n",
      "cp -r unet_4conv_sst_realworld_fixed_discrete_factor_1_seed_1/missing_90/missing_mask.npy unet_4conv_sst_realworld_fixed_discrete_factor_1_final/missing_90\n",
      "cp -r unet_4conv_sst_realworld_fixed_discrete_factor_1_seed_1/missing_75/epoch_9 unet_4conv_sst_realworld_fixed_discrete_factor_1_final/missing_75/model\n",
      "cp -r unet_4conv_sst_realworld_fixed_discrete_factor_1_seed_1/missing_75/missing_mask.npy unet_4conv_sst_realworld_fixed_discrete_factor_1_final/missing_75\n",
      "cp -r unet_4conv_sst_realworld_fixed_discrete_factor_1_seed_1/missing_50/epoch_10 unet_4conv_sst_realworld_fixed_discrete_factor_1_final/missing_50/model\n",
      "cp -r unet_4conv_sst_realworld_fixed_discrete_factor_1_seed_1/missing_50/missing_mask.npy unet_4conv_sst_realworld_fixed_discrete_factor_1_final/missing_50\n",
      "cp -r unet_4conv_sst_realworld_fixed_discrete_factor_1_seed_1/missing_25/epoch_10 unet_4conv_sst_realworld_fixed_discrete_factor_1_final/missing_25/model\n",
      "cp -r unet_4conv_sst_realworld_fixed_discrete_factor_1_seed_1/missing_25/missing_mask.npy unet_4conv_sst_realworld_fixed_discrete_factor_1_final/missing_25\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Find epch and run with minimum train loss:\n",
    "\n",
    "## First step: Find for each run the epoch with minimum train loss and the according loss values.\n",
    "\n",
    "# Initialize storage, dimensions: (#runs, #missing value settings)\n",
    "train_loss_min_epoch = np.zeros((train_loss_final_all.shape[0],train_loss_final_all.shape[1]))\n",
    "train_loss_min_value = np.zeros((train_loss_final_all.shape[0],train_loss_final_all.shape[1]))\n",
    "\n",
    "# Loop over runs:\n",
    "for i in range(train_loss_final_all.shape[0]):\n",
    "    \n",
    "    # Loop over missing value settings:\n",
    "    for j in range (train_loss_final_all.shape[1]):\n",
    "        \n",
    "        # Find epoch with minimum loss:\n",
    "        min_epoch = np.argsort(train_loss_final_all[i,j,:])[0]\n",
    "        \n",
    "        # Store epoch and according loss:\n",
    "        train_loss_min_epoch[i,j] = min_epoch\n",
    "        train_loss_min_value[i,j] = train_loss_final_all[i,j,min_epoch]\n",
    "        \n",
    "## Second step: Find the run with absolute minimum train loss.\n",
    "\n",
    "# Initialize storage, dimensions: (#missing value settings)\n",
    "train_loss_min_run = np.zeros(train_loss_final_all.shape[1])\n",
    "\n",
    "# Loop over missing value settings:\n",
    "for j in range(train_loss_final_all.shape[1]):\n",
    "    \n",
    "    # Find run with minimum loss:\n",
    "    min_run = np.argsort(train_loss_min_value[:,j])[0]\n",
    "    \n",
    "    # Store run number:\n",
    "    train_loss_min_run[j] = min_run\n",
    "\n",
    "## Output for each missing value setting the run / epoch combination with best performance (lowest train loss):\n",
    "\n",
    "# Loop over missing value settings:\n",
    "for j in range(train_loss_final_all.shape[1]):\n",
    "    \n",
    "    # Rel. amount of missing values = 0.999 requires special treatment:\n",
    "    if missing_values[j]==0.999:\n",
    "        \n",
    "        # Output: Copy optimal models\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_seed_'+str(int(train_loss_min_run[j])+1)+'/missing_' f'{int(missing_values[j]*1000)}'+\n",
    "            '/epoch_'+str(int(train_loss_min_epoch[int(train_loss_min_run[j]),j]))+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*1000)}'+'/model'\n",
    "        )\n",
    "\n",
    "        # Output: Copy missing masks\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_seed_'+str(int(train_loss_min_run[j])+1)+'/missing_' f'{int(missing_values[j]*1000)}'+\n",
    "            '/missing_mask.npy'+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*1000)}'\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Output: Copy optimal models\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_seed_'+str(int(train_loss_min_run[j])+1)+'/missing_' f'{int(missing_values[j]*100)}'+\n",
    "            '/epoch_'+str(int(train_loss_min_epoch[int(train_loss_min_run[j]),j]))+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*100)}'+'/model'\n",
    "        )\n",
    "\n",
    "        # Output: Copy missing masks\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_seed_'+str(int(train_loss_min_run[j])+1)+'/missing_' f'{int(missing_values[j]*100)}'+\n",
    "            '/missing_mask.npy'+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*100)}'\n",
    "        )\n",
    "    \n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e13bb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_realworld_variable_discrete_factor_1_seed_1\n",
      "GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_realworld_variable_discrete_factor_1_seed_2\n",
      "GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_realworld_variable_discrete_factor_1_seed_3\n"
     ]
    }
   ],
   "source": [
    "# Specific parameters for exp2:\n",
    "mask_types = ['variable','variable','variable']\n",
    "missing_types = ['discrete','discrete','discrete']\n",
    "augmentation_factors = [1,1,1]\n",
    "runs = ['_seed_1', '_seed_2', '_seed_3']\n",
    "\n",
    "# Reconstruct path to experiment:\n",
    "paths = []\n",
    "for i in range(len(mask_types)):\n",
    "    paths.append(Path('GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/'+model_config+'_'+feature_short+'_'+source+'_'\n",
    "                      +mask_types[i]+'_'+missing_types[i]+'_factor_'+str(augmentation_factors[i])+runs[i]))\n",
    "    print(paths[i])\n",
    "\n",
    "# Initialize storage for parameters of all experiments:\n",
    "parameters_all = []\n",
    "\n",
    "# Reload parameters for all experiments:\n",
    "for path in paths:\n",
    "    \n",
    "    # Open corresponding parameter file and store:\n",
    "    with open(path / 'parameters.json', 'r') as f:\n",
    "        parameters_all.append(load(f))\n",
    "\n",
    "# Convert to numpy array:\n",
    "parameters_all = np.array(parameters_all)\n",
    "\n",
    "# Extract certain parameters from first experiment. Note: These parameters should be identical for all experiments of interest, here.\n",
    "train_val_split = parameters_all[0]['train_val_split']\n",
    "missing_values = parameters_all[0]['missing_values']\n",
    "scale_to = parameters_all[0]['scale_to']\n",
    "epochs = parameters_all[0]['epochs']\n",
    "\n",
    "## Get an overview of final train and validation loss, depending on rel. amount of missing values, for all experiments.\n",
    "## Note: Start with epoch 0, hence untrained model.\n",
    "\n",
    "# Initialize storage for final train and validation loss. Dimension: (#experiments, #missing values settings, #epochs+1)\n",
    "train_loss_final_all = np.zeros((len(paths),len(missing_values),epochs+1))\n",
    "val_loss_final_all = np.zeros((len(paths),len(missing_values),epochs+1))\n",
    "\n",
    "# Loop over experiments:\n",
    "for i in range(len(paths)):\n",
    "    \n",
    "    # Get path for current experiment:\n",
    "    path = paths[i]\n",
    "\n",
    "    # Loop over array of desired rel. amounts of missing values:\n",
    "    for j in range(len(missing_values)):\n",
    "\n",
    "        # Get current sparsity:\n",
    "        missing = missing_values[j]\n",
    "\n",
    "        # Reload loss.\n",
    "        # Rel. amount of missing values = 0.999 requires special treatment:\n",
    "        if missing==0.999:\n",
    "            train_loss = np.load(path / 'missing_' f'{int(missing*1000)}'/ 'train_loss.npy')\n",
    "            val_loss = np.load(path / 'missing_' f'{int(missing*1000)}'/ 'val_loss.npy')\n",
    "        else:\n",
    "            train_loss = np.load(path / 'missing_' f'{int(missing*100)}'/ 'train_loss.npy')\n",
    "            val_loss = np.load(path / 'missing_' f'{int(missing*100)}'/ 'val_loss.npy')\n",
    " \n",
    "        # Store final train and validation loss:\n",
    "        train_loss_final_all[i,j,:] = train_loss\n",
    "        val_loss_final_all[i,j,:] = val_loss\n",
    "        \n",
    "# Get minimum train and validation loss over epochs for experiment 2:\n",
    "train_loss_min_exp2 = np.min(train_loss_final_all,axis=-1)\n",
    "val_loss_min_exp2 = np.min(val_loss_final_all,axis=-1)\n",
    "\n",
    "# Get mean and std dev of minimum train and validation loss over all runs:\n",
    "train_loss_min_mean_exp2 = np.mean(train_loss_min_exp2,axis=0)\n",
    "val_loss_min_mean_exp2 = np.mean(val_loss_min_exp2,axis=0)\n",
    "train_loss_min_std_exp2 = np.std(train_loss_min_exp2,axis=0)\n",
    "val_loss_min_std_exp2 = np.std(val_loss_min_exp2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04cf5325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_1_seed_1/missing_999/epoch_10 unet_4conv_slp_realworld_variable_discrete_factor_1_final/missing_999/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_1_seed_1/missing_999/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_1_final/missing_999\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_1_seed_3/missing_99/epoch_10 unet_4conv_slp_realworld_variable_discrete_factor_1_final/missing_99/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_1_seed_3/missing_99/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_1_final/missing_99\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_1_seed_3/missing_95/epoch_10 unet_4conv_slp_realworld_variable_discrete_factor_1_final/missing_95/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_1_seed_3/missing_95/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_1_final/missing_95\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_1_seed_1/missing_90/epoch_10 unet_4conv_slp_realworld_variable_discrete_factor_1_final/missing_90/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_1_seed_1/missing_90/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_1_final/missing_90\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_1_seed_3/missing_75/epoch_10 unet_4conv_slp_realworld_variable_discrete_factor_1_final/missing_75/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_1_seed_3/missing_75/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_1_final/missing_75\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_1_seed_1/missing_50/epoch_10 unet_4conv_slp_realworld_variable_discrete_factor_1_final/missing_50/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_1_seed_1/missing_50/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_1_final/missing_50\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Find epoch and run with minimum train loss:\n",
    "\n",
    "## First step: Find for each run the epoch with minimum train loss and the according loss values.\n",
    "\n",
    "# Initialize storage, dimensions: (#runs, #missing value settings)\n",
    "train_loss_min_epoch = np.zeros((train_loss_final_all.shape[0],train_loss_final_all.shape[1]))\n",
    "train_loss_min_value = np.zeros((train_loss_final_all.shape[0],train_loss_final_all.shape[1]))\n",
    "\n",
    "# Loop over runs:\n",
    "for i in range(train_loss_final_all.shape[0]):\n",
    "    \n",
    "    # Loop over missing value settings:\n",
    "    for j in range (train_loss_final_all.shape[1]):\n",
    "        \n",
    "        # Find epoch with minimum loss:\n",
    "        min_epoch = np.argsort(train_loss_final_all[i,j,:])[0]\n",
    "        \n",
    "        # Store epoch and according loss:\n",
    "        train_loss_min_epoch[i,j] = min_epoch\n",
    "        train_loss_min_value[i,j] = train_loss_final_all[i,j,min_epoch]\n",
    "        \n",
    "## Second step: Find the run with absolute minimum train loss.\n",
    "\n",
    "# Initialize storage, dimensions: (#missing value settings)\n",
    "train_loss_min_run = np.zeros(train_loss_final_all.shape[1])\n",
    "\n",
    "# Loop over missing value settings:\n",
    "for j in range(train_loss_final_all.shape[1]):\n",
    "    \n",
    "    # Find run with minimum loss:\n",
    "    min_run = np.argsort(train_loss_min_value[:,j])[0]\n",
    "    \n",
    "    # Store run number:\n",
    "    train_loss_min_run[j] = min_run\n",
    "\n",
    "## Output for each missing value setting the run / epoch combination with best performance (lowest train loss):\n",
    "\n",
    "# Loop over missing value settings:\n",
    "for j in range(train_loss_final_all.shape[1]):\n",
    "    \n",
    "    # Rel. amount of missing values = 0.999 requires special treatment:\n",
    "    if missing_values[j]==0.999:\n",
    "        \n",
    "        # Output: Copy optimal models\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_seed_'+str(int(train_loss_min_run[j])+1)+'/missing_' f'{int(missing_values[j]*1000)}'+\n",
    "            '/epoch_'+str(int(train_loss_min_epoch[int(train_loss_min_run[j]),j]))+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*1000)}'+'/model'\n",
    "        )\n",
    "\n",
    "        # Output: Copy missing masks\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_seed_'+str(int(train_loss_min_run[j])+1)+'/missing_' f'{int(missing_values[j]*1000)}'+\n",
    "            '/missing_mask.npy'+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*1000)}'\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Output: Copy optimal models\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_seed_'+str(int(train_loss_min_run[j])+1)+'/missing_' f'{int(missing_values[j]*100)}'+\n",
    "            '/epoch_'+str(int(train_loss_min_epoch[int(train_loss_min_run[j]),j]))+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*100)}'+'/model'\n",
    "        )\n",
    "\n",
    "        # Output: Copy missing masks\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_seed_'+str(int(train_loss_min_run[j])+1)+'/missing_' f'{int(missing_values[j]*100)}'+\n",
    "            '/missing_mask.npy'+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*100)}'\n",
    "        )\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45c8a084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_realworld_variable_discrete_factor_2_seed_1\n",
      "GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_realworld_variable_discrete_factor_2_seed_2\n",
      "GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_realworld_variable_discrete_factor_2_seed_3\n"
     ]
    }
   ],
   "source": [
    "# Specific parameters for exp3a:\n",
    "mask_types = ['variable','variable','variable']\n",
    "missing_types = ['discrete','discrete','discrete']\n",
    "augmentation_factors = [2,2,2]\n",
    "runs = ['_seed_1', '_seed_2', '_seed_3']\n",
    "\n",
    "# Reconstruct path to experiment:\n",
    "paths = []\n",
    "for i in range(len(mask_types)):\n",
    "    paths.append(Path('GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/'+model_config+'_'+feature_short+'_'+source+'_'\n",
    "                      +mask_types[i]+'_'+missing_types[i]+'_factor_'+str(augmentation_factors[i])+runs[i]))\n",
    "    print(paths[i])\n",
    "\n",
    "# Initialize storage for parameters of all experiments:\n",
    "parameters_all = []\n",
    "\n",
    "# Reload parameters for all experiments:\n",
    "for path in paths:\n",
    "    \n",
    "    # Open corresponding parameter file and store:\n",
    "    with open(path / 'parameters.json', 'r') as f:\n",
    "        parameters_all.append(load(f))\n",
    "\n",
    "# Convert to numpy array:\n",
    "parameters_all = np.array(parameters_all)\n",
    "\n",
    "# Extract certain parameters from first experiment. Note: These parameters should be identical for all experiments of interest, here.\n",
    "train_val_split = parameters_all[0]['train_val_split']\n",
    "missing_values = parameters_all[0]['missing_values']\n",
    "scale_to = parameters_all[0]['scale_to']\n",
    "epochs = parameters_all[0]['epochs']\n",
    "\n",
    "## Get an overview of final train and validation loss, depending on rel. amount of missing values, for all experiments.\n",
    "## Note: Start with epoch 0, hence untrained model.\n",
    "\n",
    "# Initialize storage for final train and validation loss. Dimension: (#experiments, #missing values settings, #epochs+1)\n",
    "train_loss_final_all = np.zeros((len(paths),len(missing_values),epochs+1))\n",
    "val_loss_final_all = np.zeros((len(paths),len(missing_values),epochs+1))\n",
    "\n",
    "# Loop over experiments:\n",
    "for i in range(len(paths)):\n",
    "    \n",
    "    # Get path for current experiment:\n",
    "    path = paths[i]\n",
    "\n",
    "    # Loop over array of desired rel. amounts of missing values:\n",
    "    for j in range(len(missing_values)):\n",
    "\n",
    "        # Get current sparsity:\n",
    "        missing = missing_values[j]\n",
    "\n",
    "        # Reload loss.\n",
    "        # Rel. amount of missing values = 0.999 requires special treatment:\n",
    "        if missing==0.999:\n",
    "            train_loss = np.load(path / 'missing_' f'{int(missing*1000)}'/ 'train_loss.npy')\n",
    "            val_loss = np.load(path / 'missing_' f'{int(missing*1000)}'/ 'val_loss.npy')\n",
    "        else:\n",
    "            train_loss = np.load(path / 'missing_' f'{int(missing*100)}'/ 'train_loss.npy')\n",
    "            val_loss = np.load(path / 'missing_' f'{int(missing*100)}'/ 'val_loss.npy')\n",
    " \n",
    "        # Store final train and validation loss:\n",
    "        train_loss_final_all[i,j,:] = train_loss\n",
    "        val_loss_final_all[i,j,:] = val_loss\n",
    "        \n",
    "# Get minimum train and validation loss over epochs for experiment 3:\n",
    "train_loss_min_exp3a = np.min(train_loss_final_all,axis=-1)\n",
    "val_loss_min_exp3a = np.min(val_loss_final_all,axis=-1)\n",
    "\n",
    "# Get mean and std dev of minimum train and validation loss over all runs:\n",
    "train_loss_min_mean_exp3a = np.mean(train_loss_min_exp3a,axis=0)\n",
    "val_loss_min_mean_exp3a = np.mean(val_loss_min_exp3a,axis=0)\n",
    "train_loss_min_std_exp3a = np.std(train_loss_min_exp3a,axis=0)\n",
    "val_loss_min_std_exp3a = np.std(val_loss_min_exp3a,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b563a4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_2_seed_2/missing_999/epoch_9 unet_4conv_slp_realworld_variable_discrete_factor_2_final/missing_999/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_2_seed_2/missing_999/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_2_final/missing_999\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_2_seed_3/missing_99/epoch_10 unet_4conv_slp_realworld_variable_discrete_factor_2_final/missing_99/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_2_seed_3/missing_99/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_2_final/missing_99\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_2_seed_2/missing_95/epoch_10 unet_4conv_slp_realworld_variable_discrete_factor_2_final/missing_95/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_2_seed_2/missing_95/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_2_final/missing_95\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_2_seed_2/missing_90/epoch_10 unet_4conv_slp_realworld_variable_discrete_factor_2_final/missing_90/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_2_seed_2/missing_90/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_2_final/missing_90\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_2_seed_1/missing_75/epoch_10 unet_4conv_slp_realworld_variable_discrete_factor_2_final/missing_75/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_2_seed_1/missing_75/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_2_final/missing_75\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_2_seed_1/missing_50/epoch_10 unet_4conv_slp_realworld_variable_discrete_factor_2_final/missing_50/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_2_seed_1/missing_50/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_2_final/missing_50\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Find epoch and run with minimum train loss:\n",
    "\n",
    "## First step: Find for each run the epoch with minimum train loss and the according loss values.\n",
    "\n",
    "# Initialize storage, dimensions: (#runs, #missing value settings)\n",
    "train_loss_min_epoch = np.zeros((train_loss_final_all.shape[0],train_loss_final_all.shape[1]))\n",
    "train_loss_min_value = np.zeros((train_loss_final_all.shape[0],train_loss_final_all.shape[1]))\n",
    "\n",
    "# Loop over runs:\n",
    "for i in range(train_loss_final_all.shape[0]):\n",
    "    \n",
    "    # Loop over missing value settings:\n",
    "    for j in range (train_loss_final_all.shape[1]):\n",
    "        \n",
    "        # Find epoch with minimum loss:\n",
    "        min_epoch = np.argsort(train_loss_final_all[i,j,:])[0]\n",
    "        \n",
    "        # Store epoch and according loss:\n",
    "        train_loss_min_epoch[i,j] = min_epoch\n",
    "        train_loss_min_value[i,j] = train_loss_final_all[i,j,min_epoch]\n",
    "        \n",
    "## Second step: Find the run with absolute minimum train loss.\n",
    "\n",
    "# Initialize storage, dimensions: (#missing value settings)\n",
    "train_loss_min_run = np.zeros(train_loss_final_all.shape[1])\n",
    "\n",
    "# Loop over missing value settings:\n",
    "for j in range(train_loss_final_all.shape[1]):\n",
    "    \n",
    "    # Find run with minimum loss:\n",
    "    min_run = np.argsort(train_loss_min_value[:,j])[0]\n",
    "    \n",
    "    # Store run number:\n",
    "    train_loss_min_run[j] = min_run\n",
    "\n",
    "## Output for each missing value setting the run / epoch combination with best performance (lowest train loss):\n",
    "\n",
    "# Loop over missing value settings:\n",
    "for j in range(train_loss_final_all.shape[1]):\n",
    "    \n",
    "    # Rel. amount of missing values = 0.999 requires special treatment:\n",
    "    if missing_values[j]==0.999:\n",
    "        \n",
    "        # Output: Copy optimal models\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_seed_'+str(int(train_loss_min_run[j])+1)+'/missing_' f'{int(missing_values[j]*1000)}'+\n",
    "            '/epoch_'+str(int(train_loss_min_epoch[int(train_loss_min_run[j]),j]))+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*1000)}'+'/model'\n",
    "        )\n",
    "\n",
    "        # Output: Copy missing masks\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_seed_'+str(int(train_loss_min_run[j])+1)+'/missing_' f'{int(missing_values[j]*1000)}'+\n",
    "            '/missing_mask.npy'+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*1000)}'\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Output: Copy optimal models\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_seed_'+str(int(train_loss_min_run[j])+1)+'/missing_' f'{int(missing_values[j]*100)}'+\n",
    "            '/epoch_'+str(int(train_loss_min_epoch[int(train_loss_min_run[j]),j]))+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*100)}'+'/model'\n",
    "        )\n",
    "\n",
    "        # Output: Copy missing masks\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_seed_'+str(int(train_loss_min_run[j])+1)+'/missing_' f'{int(missing_values[j]*100)}'+\n",
    "            '/missing_mask.npy'+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*100)}'\n",
    "        )\n",
    "        \n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87a63443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_realworld_variable_discrete_factor_3_seed_1\n",
      "GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_realworld_variable_discrete_factor_3_seed_2\n",
      "GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_realworld_variable_discrete_factor_3_seed_3\n"
     ]
    }
   ],
   "source": [
    "# Specific parameters for exp3b:\n",
    "mask_types = ['variable','variable','variable']\n",
    "missing_types = ['discrete','discrete','discrete']\n",
    "augmentation_factors = [3,3,3]\n",
    "runs = ['_seed_1','_seed_2','_seed_3']\n",
    "\n",
    "# Reconstruct path to experiment:\n",
    "paths = []\n",
    "for i in range(len(mask_types)):\n",
    "    paths.append(Path('GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/'+model_config+'_'+feature_short+'_'+source+'_'\n",
    "                      +mask_types[i]+'_'+missing_types[i]+'_factor_'+str(augmentation_factors[i])+runs[i]))\n",
    "    print(paths[i])\n",
    "\n",
    "# Initialize storage for parameters of all experiments:\n",
    "parameters_all = []\n",
    "\n",
    "# Reload parameters for all experiments:\n",
    "for path in paths:\n",
    "    \n",
    "    # Open corresponding parameter file and store:\n",
    "    with open(path / 'parameters.json', 'r') as f:\n",
    "        parameters_all.append(load(f))\n",
    "\n",
    "# Convert to numpy array:\n",
    "parameters_all = np.array(parameters_all)\n",
    "\n",
    "# Extract certain parameters from first experiment. Note: These parameters should be identical for all experiments of interest, here.\n",
    "train_val_split = parameters_all[0]['train_val_split']\n",
    "missing_values = parameters_all[0]['missing_values']\n",
    "scale_to = parameters_all[0]['scale_to']\n",
    "epochs = parameters_all[0]['epochs']\n",
    "\n",
    "## Get an overview of final train and validation loss, depending on rel. amount of missing values, for all experiments.\n",
    "## Note: Start with epoch 0, hence untrained model.\n",
    "\n",
    "# Initialize storage for final train and validation loss. Dimension: (#experiments, #missing values settings, #epochs+1)\n",
    "train_loss_final_all = np.zeros((len(paths),len(missing_values),epochs+1))\n",
    "val_loss_final_all = np.zeros((len(paths),len(missing_values),epochs+1))\n",
    "\n",
    "# Loop over experiments:\n",
    "for i in range(len(paths)):\n",
    "    \n",
    "    # Get path for current experiment:\n",
    "    path = paths[i]\n",
    "\n",
    "    # Loop over array of desired rel. amounts of missing values:\n",
    "    for j in range(len(missing_values)):\n",
    "\n",
    "        # Get current sparsity:\n",
    "        missing = missing_values[j]\n",
    "\n",
    "        # Reload loss.\n",
    "        # Rel. amount of missing values = 0.999 requires special treatment:\n",
    "        if missing==0.999:\n",
    "            train_loss = np.load(path / 'missing_' f'{int(missing*1000)}'/ 'train_loss.npy')\n",
    "            val_loss = np.load(path / 'missing_' f'{int(missing*1000)}'/ 'val_loss.npy')\n",
    "        else:\n",
    "            train_loss = np.load(path / 'missing_' f'{int(missing*100)}'/ 'train_loss.npy')\n",
    "            val_loss = np.load(path / 'missing_' f'{int(missing*100)}'/ 'val_loss.npy')\n",
    " \n",
    "        # Store final train and validation loss:\n",
    "        train_loss_final_all[i,j,:] = train_loss\n",
    "        val_loss_final_all[i,j,:] = val_loss\n",
    "        \n",
    "# Get minimum train and validation loss over epochs for experiment 3:\n",
    "train_loss_min_exp3b = np.min(train_loss_final_all,axis=-1)\n",
    "val_loss_min_exp3b = np.min(val_loss_final_all,axis=-1)\n",
    "\n",
    "# Get mean and std dev of minimum train and validation loss over all runs:\n",
    "train_loss_min_mean_exp3b = np.mean(train_loss_min_exp3b,axis=0)\n",
    "val_loss_min_mean_exp3b = np.mean(val_loss_min_exp3b,axis=0)\n",
    "train_loss_min_std_exp3b = np.std(train_loss_min_exp3b,axis=0)\n",
    "val_loss_min_std_exp3b = np.std(val_loss_min_exp3b,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da2df9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_3_seed_2/missing_999/epoch_9 unet_4conv_slp_realworld_variable_discrete_factor_3_final/missing_999/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_3_seed_2/missing_999/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_3_final/missing_999\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_3_seed_1/missing_99/epoch_10 unet_4conv_slp_realworld_variable_discrete_factor_3_final/missing_99/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_3_seed_1/missing_99/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_3_final/missing_99\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_3_seed_2/missing_95/epoch_10 unet_4conv_slp_realworld_variable_discrete_factor_3_final/missing_95/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_3_seed_2/missing_95/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_3_final/missing_95\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_3_seed_2/missing_90/epoch_10 unet_4conv_slp_realworld_variable_discrete_factor_3_final/missing_90/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_3_seed_2/missing_90/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_3_final/missing_90\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_3_seed_2/missing_75/epoch_10 unet_4conv_slp_realworld_variable_discrete_factor_3_final/missing_75/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_3_seed_2/missing_75/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_3_final/missing_75\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_3_seed_2/missing_50/epoch_10 unet_4conv_slp_realworld_variable_discrete_factor_3_final/missing_50/model\n",
      "cp -r unet_4conv_slp_realworld_variable_discrete_factor_3_seed_2/missing_50/missing_mask.npy unet_4conv_slp_realworld_variable_discrete_factor_3_final/missing_50\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Find epch and run with minimum train loss:\n",
    "\n",
    "## First step: Find for each run the epoch with minimum train loss and the according loss values.\n",
    "\n",
    "# Initialize storage, dimensions: (#runs, #missing value settings)\n",
    "train_loss_min_epoch = np.zeros((train_loss_final_all.shape[0],train_loss_final_all.shape[1]))\n",
    "train_loss_min_value = np.zeros((train_loss_final_all.shape[0],train_loss_final_all.shape[1]))\n",
    "\n",
    "# Loop over runs:\n",
    "for i in range(train_loss_final_all.shape[0]):\n",
    "    \n",
    "    # Loop over missing value settings:\n",
    "    for j in range (train_loss_final_all.shape[1]):\n",
    "        \n",
    "        # Find epoch with minimum loss:\n",
    "        min_epoch = np.argsort(train_loss_final_all[i,j,:])[0]\n",
    "        \n",
    "        # Store epoch and according loss:\n",
    "        train_loss_min_epoch[i,j] = min_epoch\n",
    "        train_loss_min_value[i,j] = train_loss_final_all[i,j,min_epoch]\n",
    "        \n",
    "## Second step: Find the run with absolute minimum train loss.\n",
    "\n",
    "# Initialize storage, dimensions: (#missing value settings)\n",
    "train_loss_min_run = np.zeros(train_loss_final_all.shape[1])\n",
    "\n",
    "# Loop over missing value settings:\n",
    "for j in range(train_loss_final_all.shape[1]):\n",
    "    \n",
    "    # Find run with minimum loss:\n",
    "    min_run = np.argsort(train_loss_min_value[:,j])[0]\n",
    "    \n",
    "    # Store run number:\n",
    "    train_loss_min_run[j] = min_run\n",
    "\n",
    "## Output for each missing value setting the run / epoch combination with best performance (lowest train loss):\n",
    "\n",
    "# Loop over missing value settings:\n",
    "for j in range(train_loss_final_all.shape[1]):\n",
    "    \n",
    "    # Rel. amount of missing values = 0.999 requires special treatment:\n",
    "    if missing_values[j]==0.999:\n",
    "        \n",
    "        # Output: Copy optimal models\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_seed_'+str(int(train_loss_min_run[j])+1)+'/missing_' f'{int(missing_values[j]*1000)}'+\n",
    "            '/epoch_'+str(int(train_loss_min_epoch[int(train_loss_min_run[j]),j]))+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*1000)}'+'/model'\n",
    "        )\n",
    "\n",
    "        # Output: Copy missing masks\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_seed_'+str(int(train_loss_min_run[j])+1)+'/missing_' f'{int(missing_values[j]*1000)}'+\n",
    "            '/missing_mask.npy'+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*1000)}'\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Output: Copy optimal models\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_seed_'+str(int(train_loss_min_run[j])+1)+'/missing_' f'{int(missing_values[j]*100)}'+\n",
    "            '/epoch_'+str(int(train_loss_min_epoch[int(train_loss_min_run[j]),j]))+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*100)}'+'/model'\n",
    "        )\n",
    "\n",
    "        # Output: Copy missing masks\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_seed_'+str(int(train_loss_min_run[j])+1)+'/missing_' f'{int(missing_values[j]*100)}'+\n",
    "            '/missing_mask.npy'+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*100)}'\n",
    "        )\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b208c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_realworld_optimal_from_CESM_discrete_factor_1_final\n"
     ]
    }
   ],
   "source": [
    "# Specific parameters for exp4a:\n",
    "mask_types = ['optimal_from_CESM']\n",
    "missing_types = ['discrete']\n",
    "augmentation_factors = [1]\n",
    "runs = ['_final']\n",
    "\n",
    "# Reconstruct path to experiment:\n",
    "paths = []\n",
    "for i in range(len(mask_types)):\n",
    "    paths.append(Path('GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/'+model_config+'_'+feature_short+'_'+source+'_'\n",
    "                      +mask_types[i]+'_'+missing_types[i]+'_factor_'+str(augmentation_factors[i])+runs[i]))\n",
    "    print(paths[i])\n",
    "\n",
    "# Initialize storage for parameters of all experiments:\n",
    "parameters_all = []\n",
    "\n",
    "# Reload parameters for all experiments:\n",
    "for path in paths:\n",
    "    \n",
    "    # Open corresponding parameter file and store:\n",
    "    with open(path / 'parameters.json', 'r') as f:\n",
    "        parameters_all.append(load(f))\n",
    "\n",
    "# Convert to numpy array:\n",
    "parameters_all = np.array(parameters_all)\n",
    "\n",
    "# Extract certain parameters from first experiment. Note: These parameters should be identical for all experiments of interest, here.\n",
    "train_val_split = parameters_all[0]['train_val_split']\n",
    "missing_values_exp4 = parameters_all[0]['missing_values']\n",
    "scale_to = parameters_all[0]['scale_to']\n",
    "epochs = parameters_all[0]['epochs']\n",
    "\n",
    "## Get an overview of final train and validation loss, depending on rel. amount of missing values, for all experiments.\n",
    "## Note: Start with epoch 0, hence untrained model.\n",
    "\n",
    "# Initialize storage for final train and validation loss. Dimension: (#experiments, #missing values settings, #epochs+1)\n",
    "train_loss_final_all = np.zeros((len(paths),len(missing_values_exp4),epochs+1))\n",
    "val_loss_final_all = np.zeros((len(paths),len(missing_values_exp4),epochs+1))\n",
    "\n",
    "# Loop over experiments:\n",
    "for i in range(len(paths)):\n",
    "    \n",
    "    # Get path for current experiment:\n",
    "    path = paths[i]\n",
    "\n",
    "    # Loop over array of desired rel. amounts of missing values:\n",
    "    for j in range(len(missing_values_exp4)):\n",
    "\n",
    "        # Get current sparsity:\n",
    "        missing = missing_values_exp4[j]\n",
    "\n",
    "        # Reload loss.\n",
    "        # Rel. amount of missing values = 0.999 requires special treatment:\n",
    "        if missing==0.999:\n",
    "            train_loss = np.load(path / 'missing_' f'{int(missing*1000)}'/ 'train_loss.npy')\n",
    "            val_loss = np.load(path / 'missing_' f'{int(missing*1000)}'/ 'val_loss.npy')\n",
    "        else:\n",
    "            train_loss = np.load(path / 'missing_' f'{int(missing*100)}'/ 'train_loss.npy')\n",
    "            val_loss = np.load(path / 'missing_' f'{int(missing*100)}'/ 'val_loss.npy')\n",
    "            \n",
    "        # Store final train and validation loss:\n",
    "        train_loss_final_all[i,j,:] = train_loss\n",
    "        val_loss_final_all[i,j,:] = val_loss\n",
    "        \n",
    "# Get minimum train and validation loss over epochs for experiment 1:\n",
    "train_loss_min_exp4a = np.min(train_loss_final_all,axis=-1)\n",
    "val_loss_min_exp4a = np.min(val_loss_final_all,axis=-1)\n",
    "\n",
    "# Get mean and std dev of minimum train and validation loss over all runs:\n",
    "train_loss_min_mean_exp4a = np.mean(train_loss_min_exp4a,axis=0)\n",
    "val_loss_min_mean_exp4a = np.mean(val_loss_min_exp4a,axis=0)\n",
    "train_loss_min_std_exp4a = np.std(train_loss_min_exp4a,axis=0)\n",
    "val_loss_min_std_exp4a = np.std(val_loss_min_exp4a,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aec33de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp -r unet_4conv_slp_realworld_optimal_from_CESM_discrete_factor_1_run_2/missing_999/epoch_9 unet_4conv_slp_realworld_optimal_from_CESM_discrete_factor_1_final/missing_999/model\n",
      "cp -r unet_4conv_slp_realworld_optimal_from_CESM_discrete_factor_1_run_2/missing_999/missing_mask.npy unet_4conv_slp_realworld_optimal_from_CESM_discrete_factor_1_final/missing_999\n",
      "cp -r unet_4conv_slp_realworld_optimal_from_CESM_discrete_factor_1_run_2/missing_99/epoch_10 unet_4conv_slp_realworld_optimal_from_CESM_discrete_factor_1_final/missing_99/model\n",
      "cp -r unet_4conv_slp_realworld_optimal_from_CESM_discrete_factor_1_run_2/missing_99/missing_mask.npy unet_4conv_slp_realworld_optimal_from_CESM_discrete_factor_1_final/missing_99\n",
      "cp -r unet_4conv_slp_realworld_optimal_from_CESM_discrete_factor_1_run_2/missing_95/epoch_8 unet_4conv_slp_realworld_optimal_from_CESM_discrete_factor_1_final/missing_95/model\n",
      "cp -r unet_4conv_slp_realworld_optimal_from_CESM_discrete_factor_1_run_2/missing_95/missing_mask.npy unet_4conv_slp_realworld_optimal_from_CESM_discrete_factor_1_final/missing_95\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Find epch and run with minimum train loss:\n",
    "\n",
    "## First step: Find for each run the epoch with minimum train loss and the according loss values.\n",
    "\n",
    "# Initialize storage, dimensions: (#runs, #missing value settings)\n",
    "train_loss_min_epoch = np.zeros((train_loss_final_all.shape[0],train_loss_final_all.shape[1]))\n",
    "train_loss_min_value = np.zeros((train_loss_final_all.shape[0],train_loss_final_all.shape[1]))\n",
    "\n",
    "# Loop over runs:\n",
    "for i in range(train_loss_final_all.shape[0]):\n",
    "    \n",
    "    # Loop over missing value settings:\n",
    "    for j in range (train_loss_final_all.shape[1]):\n",
    "        \n",
    "        # Find epoch with minimum loss:\n",
    "        min_epoch = np.argsort(train_loss_final_all[i,j,:])[0]\n",
    "        \n",
    "        # Store epoch and according loss:\n",
    "        train_loss_min_epoch[i,j] = min_epoch\n",
    "        train_loss_min_value[i,j] = train_loss_final_all[i,j,min_epoch]\n",
    "        \n",
    "## Second step: Find the run with absolute minimum train loss.\n",
    "\n",
    "# Initialize storage, dimensions: (#missing value settings)\n",
    "train_loss_min_run = np.zeros(train_loss_final_all.shape[1])\n",
    "\n",
    "# Loop over missing value settings:\n",
    "for j in range(train_loss_final_all.shape[1]):\n",
    "    \n",
    "    # Find run with minimum loss:\n",
    "    min_run = np.argsort(train_loss_min_value[:,j])[0]\n",
    "    \n",
    "    # Store run number:\n",
    "    train_loss_min_run[j] = min_run\n",
    "\n",
    "## Output for each missing value setting the run / epoch combination with best performance (lowest train loss):\n",
    "\n",
    "# Loop over missing value settings:\n",
    "for j in range(train_loss_final_all.shape[1]):\n",
    "    \n",
    "    # Rel. amount of missing values = 0.999 requires special treatment:\n",
    "    if missing_values[j]==0.999:\n",
    "        \n",
    "        # Output: Copy optimal models\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_run_2'+'/missing_' f'{int(missing_values[j]*1000)}'+\n",
    "            '/epoch_'+str(int(train_loss_min_epoch[int(train_loss_min_run[j]),j]))+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*1000)}'+'/model'\n",
    "        )\n",
    "\n",
    "        # Output: Copy missing masks\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_run_2'+'/missing_' f'{int(missing_values[j]*1000)}'+\n",
    "            '/missing_mask.npy'+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*1000)}'\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Output: Copy optimal models\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_run_2'+'/missing_' f'{int(missing_values[j]*100)}'+\n",
    "            '/epoch_'+str(int(train_loss_min_epoch[int(train_loss_min_run[j]),j]))+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*100)}'+'/model'\n",
    "        )\n",
    "\n",
    "        # Output: Copy missing masks\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_run_2'+'/missing_' f'{int(missing_values[j]*100)}'+\n",
    "            '/missing_mask.npy'+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*100)}'\n",
    "        )\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcd732ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_realworld_optimal_from_FOCI_discrete_factor_1_final\n"
     ]
    }
   ],
   "source": [
    "# Specific parameters for exp4b:\n",
    "mask_types = ['optimal_from_FOCI']\n",
    "missing_types = ['discrete']\n",
    "augmentation_factors = [1]\n",
    "runs = ['_final']\n",
    "\n",
    "# Reconstruct path to experiment:\n",
    "paths = []\n",
    "for i in range(len(mask_types)):\n",
    "    paths.append(Path('GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/'+model_config+'_'+feature_short+'_'+source+'_'\n",
    "                      +mask_types[i]+'_'+missing_types[i]+'_factor_'+str(augmentation_factors[i])+runs[i]))\n",
    "    print(paths[i])\n",
    "\n",
    "# Initialize storage for parameters of all experiments:\n",
    "parameters_all = []\n",
    "\n",
    "# Reload parameters for all experiments:\n",
    "for path in paths:\n",
    "    \n",
    "    # Open corresponding parameter file and store:\n",
    "    with open(path / 'parameters.json', 'r') as f:\n",
    "        parameters_all.append(load(f))\n",
    "\n",
    "# Convert to numpy array:\n",
    "parameters_all = np.array(parameters_all)\n",
    "\n",
    "# Extract certain parameters from first experiment. Note: These parameters should be identical for all experiments of interest, here.\n",
    "train_val_split = parameters_all[0]['train_val_split']\n",
    "missing_values_exp4 = parameters_all[0]['missing_values']\n",
    "scale_to = parameters_all[0]['scale_to']\n",
    "epochs = parameters_all[0]['epochs']\n",
    "\n",
    "## Get an overview of final train and validation loss, depending on rel. amount of missing values, for all experiments.\n",
    "## Note: Start with epoch 0, hence untrained model.\n",
    "\n",
    "# Initialize storage for final train and validation loss. Dimension: (#experiments, #missing values settings, #epochs+1)\n",
    "train_loss_final_all = np.zeros((len(paths),len(missing_values_exp4),epochs+1))\n",
    "val_loss_final_all = np.zeros((len(paths),len(missing_values_exp4),epochs+1))\n",
    "\n",
    "# Loop over experiments:\n",
    "for i in range(len(paths)):\n",
    "    \n",
    "    # Get path for current experiment:\n",
    "    path = paths[i]\n",
    "\n",
    "    # Loop over array of desired rel. amounts of missing values:\n",
    "    for j in range(len(missing_values_exp4)):\n",
    "\n",
    "        # Get current sparsity:\n",
    "        missing = missing_values_exp4[j]\n",
    "\n",
    "        # Reload loss.\n",
    "        # Rel. amount of missing values = 0.999 requires special treatment:\n",
    "        if missing==0.999:\n",
    "            train_loss = np.load(path / 'missing_' f'{int(missing*1000)}'/ 'train_loss.npy')\n",
    "            val_loss = np.load(path / 'missing_' f'{int(missing*1000)}'/ 'val_loss.npy')\n",
    "        else:\n",
    "            train_loss = np.load(path / 'missing_' f'{int(missing*100)}'/ 'train_loss.npy')\n",
    "            val_loss = np.load(path / 'missing_' f'{int(missing*100)}'/ 'val_loss.npy')\n",
    "            \n",
    "        # Store final train and validation loss:\n",
    "        train_loss_final_all[i,j,:] = train_loss\n",
    "        val_loss_final_all[i,j,:] = val_loss\n",
    "        \n",
    "# Get minimum train and validation loss over epochs for experiment 1:\n",
    "train_loss_min_exp4b = np.min(train_loss_final_all,axis=-1)\n",
    "val_loss_min_exp4b = np.min(val_loss_final_all,axis=-1)\n",
    "\n",
    "# Get mean and std dev of minimum train and validation loss over all runs:\n",
    "train_loss_min_mean_exp4b = np.mean(train_loss_min_exp4b,axis=0)\n",
    "val_loss_min_mean_exp4b = np.mean(val_loss_min_exp4b,axis=0)\n",
    "train_loss_min_std_exp4b = np.std(train_loss_min_exp4b,axis=0)\n",
    "val_loss_min_std_exp4b = np.std(val_loss_min_exp4b,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3335525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp -r unet_4conv_slp_realworld_optimal_from_FOCI_discrete_factor_1_run_2/missing_999/epoch_9 unet_4conv_slp_realworld_optimal_from_FOCI_discrete_factor_1_final/missing_999/model\n",
      "cp -r unet_4conv_slp_realworld_optimal_from_FOCI_discrete_factor_1_run_2/missing_999/missing_mask.npy unet_4conv_slp_realworld_optimal_from_FOCI_discrete_factor_1_final/missing_999\n",
      "cp -r unet_4conv_slp_realworld_optimal_from_FOCI_discrete_factor_1_run_2/missing_99/epoch_10 unet_4conv_slp_realworld_optimal_from_FOCI_discrete_factor_1_final/missing_99/model\n",
      "cp -r unet_4conv_slp_realworld_optimal_from_FOCI_discrete_factor_1_run_2/missing_99/missing_mask.npy unet_4conv_slp_realworld_optimal_from_FOCI_discrete_factor_1_final/missing_99\n",
      "cp -r unet_4conv_slp_realworld_optimal_from_FOCI_discrete_factor_1_run_2/missing_95/epoch_9 unet_4conv_slp_realworld_optimal_from_FOCI_discrete_factor_1_final/missing_95/model\n",
      "cp -r unet_4conv_slp_realworld_optimal_from_FOCI_discrete_factor_1_run_2/missing_95/missing_mask.npy unet_4conv_slp_realworld_optimal_from_FOCI_discrete_factor_1_final/missing_95\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Find epch and run with minimum train loss:\n",
    "\n",
    "## First step: Find for each run the epoch with minimum train loss and the according loss values.\n",
    "\n",
    "# Initialize storage, dimensions: (#runs, #missing value settings)\n",
    "train_loss_min_epoch = np.zeros((train_loss_final_all.shape[0],train_loss_final_all.shape[1]))\n",
    "train_loss_min_value = np.zeros((train_loss_final_all.shape[0],train_loss_final_all.shape[1]))\n",
    "\n",
    "# Loop over runs:\n",
    "for i in range(train_loss_final_all.shape[0]):\n",
    "    \n",
    "    # Loop over missing value settings:\n",
    "    for j in range (train_loss_final_all.shape[1]):\n",
    "        \n",
    "        # Find epoch with minimum loss:\n",
    "        min_epoch = np.argsort(train_loss_final_all[i,j,:])[0]\n",
    "        \n",
    "        # Store epoch and according loss:\n",
    "        train_loss_min_epoch[i,j] = min_epoch\n",
    "        train_loss_min_value[i,j] = train_loss_final_all[i,j,min_epoch]\n",
    "        \n",
    "## Second step: Find the run with absolute minimum train loss.\n",
    "\n",
    "# Initialize storage, dimensions: (#missing value settings)\n",
    "train_loss_min_run = np.zeros(train_loss_final_all.shape[1])\n",
    "\n",
    "# Loop over missing value settings:\n",
    "for j in range(train_loss_final_all.shape[1]):\n",
    "    \n",
    "    # Find run with minimum loss:\n",
    "    min_run = np.argsort(train_loss_min_value[:,j])[0]\n",
    "    \n",
    "    # Store run number:\n",
    "    train_loss_min_run[j] = min_run\n",
    "\n",
    "## Output for each missing value setting the run / epoch combination with best performance (lowest train loss):\n",
    "\n",
    "# Loop over missing value settings:\n",
    "for j in range(train_loss_final_all.shape[1]):\n",
    "    \n",
    "    # Rel. amount of missing values = 0.999 requires special treatment:\n",
    "    if missing_values[j]==0.999:\n",
    "        \n",
    "        # Output: Copy optimal models\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_run_2'+'/missing_' f'{int(missing_values[j]*1000)}'+\n",
    "            '/epoch_'+str(int(train_loss_min_epoch[int(train_loss_min_run[j]),j]))+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*1000)}'+'/model'\n",
    "        )\n",
    "\n",
    "        # Output: Copy missing masks\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_run_2'+'/missing_' f'{int(missing_values[j]*1000)}'+\n",
    "            '/missing_mask.npy'+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*1000)}'\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Output: Copy optimal models\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_run_2'+'/missing_' f'{int(missing_values[j]*100)}'+\n",
    "            '/epoch_'+str(int(train_loss_min_epoch[int(train_loss_min_run[j]),j]))+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*100)}'+'/model'\n",
    "        )\n",
    "\n",
    "        # Output: Copy missing masks\n",
    "        print(\n",
    "            'cp -r '+model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_run_2'+'/missing_' f'{int(missing_values[j]*100)}'+\n",
    "            '/missing_mask.npy'+' '+\n",
    "            model_config+'_'+feature_short+'_'+source+'_'+mask_types[0]+'_'+missing_types[0]+'_factor_'+\n",
    "            str(augmentation_factors[0])+'_final'+'/missing_' f'{int(missing_values[j]*100)}'\n",
    "        )\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9369b9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a6045a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAEeCAYAAACAIyQ5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABncklEQVR4nO3dd5gUVdbH8e8ZoiBJQASBARMZUVhERVExB1hZRVlQEJAXRUXRFRF1MWBiFXMAxDUgJgy7ZkBAcUUZgoBKMMwAgoIkBSSf94+qwWaYGXpCT034fZ6nnumqulV1qqunT9+qW7fM3REREREREZH4JUUdgIiIiIiISFGjipSIiIiIiEgOqSIlIiIiIiKSQ6pIiYiIiIiI5JAqUiIiIiIiIjmkipSIiIiIiEgOqSJVRJjZX83sEzNbZWZ/mFmamb1lZmfGlDnJzNzMTs1mPb3CMunD72b2lZldZWalC2Zvsmdm/zaz1KjjyEzM+9dgH+UahOV65XI76cfn7kzmmZn9EM5/MTfrT7Rw/4eZ2SEJWHf65/ykXCw7zMwK/JkP8X5uRAor5aDCQTkoGmY21cymxozHnYfCcsNysc1eZtY7i+kFnk8yvgcSUEWqCDCza4A3gSVAH+Ac4K5w9im5XO2FwLHA34AvgUeB2/IWqeSz34HuZmYZpp8ANAA2FXhE8WsA/BPI94oUMJvgszs7F8uOCZcVkTgpB5VYRTkHJVpe8lC8egF7VaSAd8Ntr0zgtiVOheLsj+zTDcBb7t4nZtrHwGgzy21leK67fxe+/sjMDgOuJYeJzMzKufvWXMZQZJhZGWBHAW/2TaAH0AGYGjP9UmAa0LCA40mIMEmXcfdt8ZR399+AGbnZlrsvB5bnZlmREkw5KGLKQYVLXvJQPmx7NbA6im3L3nRFqmg4APg5sxnuviuftjETqGRmB2ZVILysO93MzjOzOWa2FbgynNfQzMaZ2Woz22pmc83s/AzLH2ZmL5jZj2HTkB/M7Ekzq5bTYM3sHTObFDNuMduuEDN9nJl9GTNexszuMrNUM9sW/r0rTFLpZdKbRFxpZveb2QpgK1A1i1gqmNkTZrbGzDaa2X+Aujndp0wsI0hWl8RsqzxwAfB8FrHUCN/Tn8L3YqGZ9ctQpqaZPW1mi81ss5ktM7OXzOzgDOWGhe/D4Wb2brhvaWZ2W3Y/nsKmDlPC0YkxTUROCuenmtmLZtbbzBYC2wjOcGNmt5vZbDPbYGa/mtnHZtYu4/ozNqmI+WyeGi6/2cwWmNlfM9unDNM8/AxcE342fzezaWbWLEO5UmG5leH6Pzazxpb7ZhvxfBZLm9mdZva9mW0J35PpZtY+pszfw//HjeH7Nt/M/i+n8YhkQzlo71iUgzKPpTDkoHJmttbMHshk3kXhOluF438xs9fNbHn4mVhkZneb2X7ZvTGWeR7KmCOmZswjYbl9fg4taELXATje/syhU8N5ezXty+Hn6v/M7I4wzvVm9l8zy9Xnxcwamdmb4Xr+MLMZFtPcNyxzRFhmlQV5bKmZvWZhU14z29/MHg2nbzWzX8xskpk1zk1MBU0VqaLhS6Cnmf3DzI5I0DYaAjuBjfsodwTwCEEzjDOAyWZWD/gCOBK4DuhEcLl7gpl1ilm2DsHVgGvDZe8AOgLv5SLej4HjzKxcON4SqA440D6m3Mn8+aMe4DngJoIkcC7wLDA4nJ7R0HB/+wHnA1uyiOVpoC/wINAFWAS8lLFQzBdvr33v3m7PAxfEfKn/FSgDvJ7J+isDnxFUSoaFf/8LPGlmV8cUPSDclyHAmcA/gMOBz8IkmdGbBO/3X4G3gNuBntnEPBsYEL6+hqAJQsYmECcDg8J1nQnMC6cfDIwMt9ULWAV8YmYts9leukOBh/nzOKwEXrfgTPe+9CB4vwYClwH1gbdtz3s2bgduJjgmnYEPgf/Ese6sxPNZHEzwP/UIwf/MZcBkgmOIBRWqFwl+7PyVoLnUaLL4wSWSS8pBe1MO2nv9hSIHhVcoXwX+bmalMszuASxw97nheH1gLtA/jOVhguZ0z2a1/mwMI8gR48JYPyLzHBHP5/BKYA5BbkzPoVdms+2cfK6GAIcR7OfAcN3j9rVzGZlZHWA6wf/dVUBXYD3wrpmdFVP0HYLcfgXB/t5EcGIgvQ4yMlz2duA0gmMxl6KSx9xdQyEfCL5I5xF8QTvwKzAeOD1DuZPC+adms65eYZlGBE07qwH/R5DA3tpHHFOBXUCrDNOfIbjMXD3D9IkEzTeyWl9pgoTjwFEx0/8NpO4jlqPC5TqE49eG79FE4J5wWuOwzJnhePNwfFiGdd0STm8ZjjcIx2cDlsX71yAcbxS+dzdlKPdkWK5XzLQOBE0zLo3jmDvBPQj7E7RDvzic/h4wLnydCrwYs8ytBMnp8AzrGh1+Zkpnsa1SQL1wm+fHTB8WTrssQ/n5wEf7iD/Lz2IY92bgoH2so1T4GVkEPJzJuk/K8NncHrvvwIHhsbk54z5l8l4vIWhemD7tgnD6ceF4NYIfeE9kWHZQZp+pbP7v0j838X4W3wHeyGa9NwBr9/V50qAhLwPKQZktqxxUuHPQ8eGyZ8RMq0mQJ27MYhkLPxM9ws9Z9Zh5U4GpmXzWTwrH03PEUxnWOTizYx7n53AqMD2b/6H0z0BOP1fTMpS7IZxeJ47/v9j34F/h5+mwDMdyETA7HK8RrrtTNutdADy4r89kYR10RaoIcPfFBF/aHYDhBDX184EPzeyWXK52IcEXylrgCYKzEZnd1JhRqv95JifdmQRfrhssaIpUOjyT/yFwZHiWCjMra2Y3W3Cp/49w+5+G62iUw/i/CmNPv9H5FIIzVh9nmBa7jRPDvxl7GUof75Bh+lse/pdn4xiCsyqvZpj+csaC7j7N3Uu7e6ZNIjLj7hsJzsZdYmYHAaeTRZMKguPwBfBjJsehOtA0vaCZXWFBT1kbCb4Il4azMjsO72YYX0BwFi8vZrj7Xk2FLGiaN8XM1oRxbSf4ERfP52OJuy9JH3H3VQRXtOKJdaK7b48Znx/+TV+2BVAReC3DcnudlY1TvJ/FmcDZZjbczNqbWdkM5WcC1SxoKnmumVXNZTwiWVIOypRy0N4KTQ5y98+A74lplghcTPBe7b76YmaVzew+M/ue4CrJduAFgkrV4dltI4P0HLHP45DPn0PI+ecq4/uZMd/lZLsz/M97HXH3nQQnWVqF/3drgB+Ae83scjPL7D2dCfQK35M2mVxFLNRUkSoi3H2nu3/i7re4+6kEvaHNB/5puWjfTZAE/0Jwxqyiu1/q7mvjWC6zXmIOJLj5dHuGYUQ4v3r49x6CM0wvElzyb0vQDAEgs8v5WfKgXf404OTwn+5EguYTU4DW4T/wycBMd0/vWeiALPbh5wzzyaJcZmqHf3/JMD3jeF48T5C8riOoGEzKotyBBO9DxuOQ/uO/OkDYxOKJcD1dCI5D+n1ImR2HjJ+LrVmUy4m93lszO5rgx9BGgp7B2hF8Rr+Kc3uZfX7jjTWzfSRm2fTjvCpDudwe53g/i3cT9H7YiSDRrjGzZ82sBgQ/jAia89Uj+LGzOmxbHk9TSJG4KQftSTkoU4UtB70InG9m+4fjlwAfu/tPMWWeJWhK9ghBs7K/8GfT9Jx8JnJyHPLtcxjK6edqX/kuJ9vN7DP6M0FFtFp4IuA0IIVgvxdbcE/YFTHlryZontqboFK1ysxGWsy9hoWZeu0rotx9hZmNIWjPezhBG/acWBB7FiEnm85k2hqCH3n3ZbHMivDvxcDz7p7ebS4xX3C5MYXg0nJ7oBJBUvudoBlCB4JL70/HlE//8jiI4EwVMeMQ7EesfZ0JhD+/RGoRnHUhZjy/TCJIXjcQXP7emUW5NWG5gVnMXxT+vRiY7O7Xp88ws4LufSmz9/ZvBGcmu8ReHQp/pK0voLiykn6cDwS+jpme2+Mc12cxfB/uA+4LzwafS3AfRAXgorDM6wT3gu1P8Jm/D/jAzOp6/nUEILIH5SBAOSijwpaDXiA4EXW+mX1BUEnqGbPN8gT3uw5z94djprfIxbZij8O+ckR+fw5z+rnKL2tjthHrIILP7loAd/8BuNTMjD/vp3rCzFLd/f3wqucQYIiZJRM0rb+XoCOqwQmKPd/oilQREN5Im5n0Hk0y7U2pAH1AcKPt1+6eksmQfrajAsHZqViX5WG7U4CyBO2yZ7v7+vAL/lOCL/IaBM0s0k0L/16cYT3dw7+f5CKGLwjaUnfNMD3jNnIt/DF8J8FNu2OzKfoBwWdiaRbH4fewXH4fh8ykH/Nsez7KoAJBW//dPx7M7BTy3owwP8wn+HF0YYbpGcfjlePPorv/7O5jCH7UNM9k/kZ3f4fgh1tt/jwLL5InykFZUg7aU2HKQbj798DnBFeiLiH4Dn8jpkg5gnt6MsbSKxebmxeuP57jEO/+byW+HJqIz1U8pgHtbM/eA0sRnOSbE3O8geDm5LBZ7qBwUmZ5LM3dHyDIuXvNL4x0RapoWGBmUwia7vwIVAbOJrgc/aq7L81Q/oRM7pXY4e5vJSi+2wjORn5iZo8R3IBajeCf4BB3T2/3/gFBz0/zge8ILmUfl9uNuvsCM1tF0NvNiJhZ6WcJtxJ8iaaX/9rMxgPDwnbb/yPoreZWYLy7zyOH3H2Rmb0E3GFBd6wzCS5jn52xrJl1IOhxrXdO2qiH23kKeGofxUYSfIF9amYjCc7+VSRIbCe4e+ew3AfAYDO7meC4nUJwBig/LSa4utTbzNYSHItFGb9YM/iA4Ibtf5vZswT3Rt0K/JTNMgXC3deZ2UPAzWb2O0Fl5miCJogQ/JDJyfri+iya2dsETRtnA+sI7lM5k/Ast5ndQXDGcwrBWfe6BD0lzvXgWSMi+UE5KBPKQXspTDko3fPA4wT3ML0ZXv0AwN03mNkM4HozW0nQIUZvgh7mcsTd14f7PDTMER8RXAHrk0nxeD+H3wBXmtlFBFeafnf3RRkLJeJzFaeRBJXOiWb2T+A3gp4Fj+DPR5q0JLhq/QrBvpYKl9lBeJLBzD4n6N1wPkHT/g4EV64y63Gw0FFFqmgYTPClmP6jaSfBD9WbgIcyKZ/ZAw03EfS+k+/cfamZtSFo83s3Qc84awhuCI39R7iaoN3s8HD8PaAbOW8SEmsqwRmg2LN+6a9nuPsfGcr3JGj+0JugR5sVBM1Bbs9DDP9H8M9/A8HZyY+BvxN0CxrLCL5EEnIlOEwKxxEc/8EEyWA9QTKbEFP0DoJuRa8jaBM9jaBL0thmIXmNZY2ZXRXGMY1gv09mz4c6ZlzmQzO7huBs1d8IPj+XEhynwuCfBMewD0Fl5QuChPAZsCEX64vns/gJwVWvAQRnMZcC9/Pn/9AXYSwjCdqrryJI4LfmIh6RrCgHZW0qykFA4cpBMV4h+CF/EEFTv4y6EfRw+DjwB0FnEQMJekzNqWEE73FfguZrXwDnsWdTP4j/c3gfQecTYwj+d6YRNBfNTCI+V9kKm/e2D7fzJMEVvrnAOe7+QVjsZ4K8NYjgRN8WggrTue4+KyzzCcH/0E0E9ZIfgOvc/ZFExZ6fzPfZIYyIiGTGzC4kSLwnuvun+yovIiIixYcqUiIicTCzYwiaK3xBcFatNcEZtEUEz5vSl6mIiEgJoqZ9IiLx2UjQte8AgntEVhFcjRqiSpSIiEjJoytSIiIiIiIiOaTuz0VERERERHKoxDbtq1Gjhjdo0CDqMERESrRZs2b96u41o46jMFKeEhGJXnZ5qsRWpBo0aEBKSkrUYYiIlGhmlhZ1DIWV8pSISPSyy1Nq2iciIiIiIpJDqkiJiIiIiIjkkCpSIiIiIiIiOVRi75ESkZJr+/btLF++nC1btkQdSolRvnx56tatS5kyZaIORUSk0FOeKni5yVOqSIlIibN8+XIqVapEgwYNMLOowyn23J01a9awfPlyGjZsGHU4IiKFnvJUwcptnlLTvhwaN24cDRo0ICkpiQYNGjBu3LioQxKRHNqyZQvVq1dXciogZkb16tV1ZrWAKE+JFH3KUwUrt3kq8oqUmY0ws4VmNs/M3jSzqpmUqWdmU8zsWzP72swGxsw7wMwmmtmS8G+1RMU6btw4+vXrR1paGu5OWloa/fr1U5ISKYKUnAqW3u+CoTwlUnzoe7Ng5eb9jrwiBUwEmrt7S2AxMCSTMjuA6929CdAOGGBmTcN5NwGT3f1wYHI4nhBDhw5l8+bNe0zbvHkzQ4cOTdQmRURE4qY8JSJScCKvSLn7R+6+IxydAdTNpMxKd58dvv4d+BY4OJzdGXgufP0c8NdExbp06dIcTRcRKSipqak0b948R8u89tprNGnShJNPPpmUlBSuueaaSOKQ/KM8JSKFVXHMU5FXpDLoDbyfXQEzawAcBXwRTqrl7ishqHABB2azbD8zSzGzlNWrV+c4uPr16+douogUD8X1npNnnnmGJ554gilTptCmTRseeeSRqEOSPFKeEimZlKeiUSAVKTObZGYLMhk6x5QZStCEL8sjb2b7AxOAa939t5zG4e6j3L2Nu7epWbNmjvdj+PDhVKhQYY9pSUlJ3HnnnTlel4gUDYm45yQ1NZXGjRvTt29fmjdvTvfu3Zk0aRLHH388hx9+OF9++SUAX375JccddxxHHXUUxx13HIsWLQLg66+/pm3btrRq1YqWLVuyZMmSPdb/ww8/cNRRRzFz5swsY7jjjjuYPn06/fv35x//+AdTp07l3HPPBeCaa67hjjvuAODDDz/kxBNPZNeuXcyaNYsOHTrQunVrzjjjDFauXAnArFmzOPLIIzn22GN5/PHHc/2+SN5llqdKlSrF8OHDI4pIRBJNeSrCPOXukQ9AT+BzoEI2ZcoAHwKDMkxfBNQOX9cGFsWzzdatW3tuvPjii56cnOxm5tWrV3fAb7311lytS0Si8c033+wx3qFDh72Gxx9/3N3d69Wr58BeQ/Xq1d3dffXq1Xstuy8//vijlypVyufNm+c7d+70o48+2i+77DLftWuXv/XWW965c2d3d9+wYYNv377d3d0nTpzoXbp0cXf3q666yl988UV3d9+6datv3rzZf/zxR2/WrJkvXLjQW7Vq5XPmzHF3959++snPOuusTOPo0KGDz5w5093dp0yZ4uecc467u2/atMmbNm3qH3/8sR9xxBH+3Xff+bZt2/zYY4/1VatWubv7yy+/7Jdddpm7u7do0cKnTp3q7u433HCDN2vWLK733d0dSPFCkIcK45AfeapatWoO+FNPPZWrdYlINJSn/tzvwpynIn+OlJmdCQwGOrj75izKGPAM8K27P5hh9n8IKmL3hn/fTmC4dO/ene7du+8e7927N8uXL8fd1buKSDG0fPnyTKevWbMmT+tt2LAhLVq0AKBZs2Z07NgRM6NFixakpqYCsGHDBnr27MmSJUswM7Zv3w7Asccey/Dhw1m+fDldunTh8MMPB2D16tV07tyZCRMm0KxZMwDq1KnDe++9l6PYKlSowOjRoznxxBMZOXIkhx56KAsWLGDBggWcdtppAOzcuZPatWuzYcMG1q9fT4cOHQC45JJLeP/9bFtoS4LF5qldu3Zx1llnMXnyZPr166c8JVIMKU9Fl6cir0gBjwHlgInhF/wMd+9vZnWAMe5+NnA8cAkw38zmhsvd7O7vEVSgXjWzPsBS4MKCDH7UqFGULl0Y3kYRya2pU6dmOa9+/fqkpaXtNT05ORmAGjVqZLt8VsqVK7f7dVJS0u7xpKQkduwI+t+59dZbOfnkk3nzzTdJTU3lpJNOAuDvf/87xxxzDO+++y5nnHEGY8aM4ZBDDqFKlSrUq1ePzz77bHeCyq358+dTvXp1VqxYAQStF5o1a8bnn3++R7n169frx3khlpSUxIQJE6hYsaKOk0gRpjy1t8KQpyLvbMLdD3P3eu7eKhz6h9NXhJUo3H26u5u7t4wp9144b427d3T3w8O/awsy/vRK1Ndff03Pnj3Ztm1bQW5eRBIss3tOKlSoUCD3nGzYsIGDDw46KP33v/+9e/oPP/zAIYccwjXXXEOnTp2YN28eAGXLluWtt97i+eef56WXXsr1dtPS0njggQeYM2cO77//Pl988QWNGjVi9erVuxPU9u3b+frrr6latSpVqlRh+vTpAMXmBufiZP/998fMSE1N3eNzJCLFg/JUdHkq8opUcfHNN9/w/PPPc9NNCXuMlYhEoHv37owaNYrk5GTMjOTkZEaNGrVHE99EufHGGxkyZAjHH388O3fu3D39lVdeoXnz5rRq1YqFCxdy6aWX7p5XsWJF3nnnHUaOHMnbb7/NihUrOPvss+PeprvTp08f/vWvf1GnTh2eeeYZ+vbty65du3j99dcZPHgwRx55JK1ateJ///sfAM8++ywDBgzg2GOPZb/99su/N0Dy1f3330+fPn12HzcRKR6Up6LLUxbcQ1XytGnTxlNSUvJ1nddccw2PPvooEyZMoEuXLvm6bhHJP99++y1NmjSJOowSJ7P33cxmuXubiEIq1PI7T/3222+0atUKgK+++opKlSrl27pFJH8pT0Ujp3lKV6Ty0b/+9S/atm3LZZddxnfffRd1OCIiIrtVrlyZF154gbS0NAYOHBh1OCIiRZ4qUvmobNmyvPrqq5QqVYp77rkn6nBERET2cPzxx3PzzTfz7LPP8p///CfqcEREijR1N5fPkpOTmTp1Ko0bN446FBHJhh5ZULBKajPywui2226jbNmynHzyyVGHIiLZUJ4qWLnJU7oilQAtW7akbNmyrF27lo8//jjqcEQkg/Lly7NmzRr9uC8g7s6aNWsoX7581KEIUKZMGW699VYqVarEli1b2LVrV9QhiUgGylMFK7d5SlekEmjgwIG88cYbfPnll3nuK19E8k/dunVZvnw5q1evjjqUEqN8+fLUrVs36jAkxurVqznppJPo378/V199ddThiEgM5amCl5s8pV77EmjlypUcddRRVKtWjZkzZ7L//vsndHsiIkWNeu3LWqLzlLtz3nnnMXnyZFJSUnTCT0QkE+q1LyK1a9dm/PjxLF68mH79+unyrIiIFBpmxjPPPEOlSpXo0aMHW7dujTokEZEiRRWpBDv55JO54447GD9+PM8880zU4YiIiOxWq1YtnnnmGebOncttt90WdTgiIkWK7pEqAEOGDGHLli2ce+65UYciIiKyh/POO49+/foxdepUtm3bRtmyZaMOSUSkSFBFqgAkJSVx5513ArBjxw62bNmi+6VERKTQeOihhyhdujRlypSJOhQRkSJDTfsK0K5duzjnnHPo0aOH7pcSEZFCY7/99qNMmTKsW7eOJ554IupwRESKBFWkClBSUhJnnXUWb7/9Ng888EDU4YiIiOxh7NixDBgwgPHjx0cdiohIoaeKVAEbOHAgf/vb37jpppuYPn161OGIiIjsNnDgQI499liuuOIKli1bFnU4IiKFmipSBSy9u9mGDRty0UUXsWrVqqhDEhERAaB06dK88MIL7Ny5k549e7Jr166oQxIRKbQir0iZ2QgzW2hm88zsTTOrmkmZemY2xcy+NbOvzWxgzLxhZvaTmc0Nh7MLdAdyoUqVKrz22mtUrlxZFSkRESlUDj30UB5++GGmTJnCo48+GnU4IiKFVmHotW8iMMTdd5jZfcAQYHCGMjuA6919tplVAmaZ2UR3/yacP9Ld/1WAMedZq1atWLBgAaVKlYo6FBERkT1cdtll/PLLL3Tt2jXqUERECq3Ir0i5+0fuviMcnQHUzaTMSnefHb7+HfgWOLjgokyMUqVKsW3bNgYOHMjEiROjDkdERAQImqEPGTKE2rVrs3PnTrZt2xZ1SCIihU7kFakMegPvZ1fAzBoARwFfxEy+KmwaONbMqmWzbD8zSzGzlNWrV+dLwHm1fft2Jk+eTPfu3fnpp5+iDkdERHIgr83TC7tt27Zx2mmnceONN0YdiohIoVMgFSkzm2RmCzIZOseUGUrQhG9cNuvZH5gAXOvuv4WTnwQOBVoBK4Es+xV391Hu3sbd29SsWTPvO5YPKlasyGuvvcbmzZu56KKL2L59e9QhiYhI/CYCzd29JbCYoHl6RunN05sA7YABZta0AGPMtbJly9KsWTMefvhhtZwQEclgnxUpMytlZt+bWbncbsTdT3X35pkMb4fb6AmcC3T3LJ5Ua2ZlCCpR49z9jZh1/+LuO919FzAaaJvbOKPSpEkTRo8ezWeffcbNN98cdTgiIhKnktA8/b777qNJkyb06tWLNWvWRB2OiEihsc+KlLvvBHYC5RMRgJmdSdC5RCd335xFGQOeAb519wczzKsdM3o+sCARcSZat27duOKKK3jyySdZsWJF1OGIiEjO5bZ5euz8QtcEvUKFCrz44ousXr2a/v37k8X5ThGREifepn0PAa+aWQczO9TMDkkf8iGGx4BKwMSw+/KnAMysjpm9F5Y5HrgEOCWTbs7vN7P5ZjYPOBm4Lh9iisTIkSNJSUmhTp06UYciIiKhBDdP30NhbIIOcPTRR3PnnXeSkpJCYangiYhEzeI5s2RmWT2Rz929SPbf3aZNG09JSYk6jEy5OxMmTODcc8+lfPmEXAgUESkUzGyWu7eJOo68CJun9wc6ZtOyogzwDvBhxpYVWSlseWrnzp1s2rSJypUrRx2KiEiByS5PxXVFyt2TshiKZCWqsJszZw4XXnghgwYNijoUERHJRl6bpxclpUqVonLlymzdupVHH32UnTt3Rh2SiEikctRrn5nVN7NjzaxeogKSoAnFDTfcwJNPPslLL70UdTgiIpK1vDZPL3Lee+89rrnmGu67776oQxERiVS8TftqAy8DxwJrgOoEvRNd7O5FsmeEwtZkIqPt27dzyimnMGfOHGbOnEmTJk2iDklEJN8Vh6Z9iVJY85S7061bNyZMmMDnn39OmzY6fCJSfOW5aR/Bs5q+Aqq5e22gGjAHeCp/QpSMypQpw8svv0yFChW48MIL9XwpEREpFMyMJ598koMOOogePXqweXOmLRpFRIq90nGWaw/UdvftAO6+ycxuBH5KWGTCwQcfzPjx4/n9998pU6ZM1OGIiIgAUK1aNZ577jk6duzITTfdxCOPPBJ1SCIiBS7eitQ6oCnBVal0jYD1+R2Q7Kljx467X69du5YDDjggwmhEREQCp5xyCo888sgeeUpEpCSJt2nf/cAkM7vXzK4ws3uBieF0KQBvvvkmDRo0YO7cuVGHIiIiAsDVV19N06ZNAdi6dWvE0YiIFKx4uz8fDVwE1ADOC/92c/dRCYxNYhx//PFUqlSJCy64gA0bNkQdjoiIyG59+vSha9euxNOBlYhIcbHPipSZlTKz74HP3L2vu58d/v24AOKT0IEHHsgrr7xCamoqvXv3VrISEZFCo3nz5vznP//hmWeeiToUEZECs8+KlLvvBHYC5RMfjmSnffv23Hvvvbzxxhs8/PDDUYcjIiICwMCBA+nYsSMDBw5kyZIlUYcjIlIg4r1H6iHgVTPrYGaHmtkh6UMCY5NMXH/99XTp0oVNmzZFHYqIiAgASUlJ/Pvf/6ZcuXL06NFDj+wQkRIh3l77Hgv/npZhugOl8i8c2Rcz47XXXiMpKd46sIiISOLVrVuXp59+mv/7v/9j4cKFtGjRIuqQREQSKp57pAw4HCjr7kkZBlWiIpBeiZo8eTKXXnopu3btijgiERERuPDCC/n+++9ViRKREiGee6QcmAfo13ohs3jxYl544QXuueeeqEMREREBgof17tq1i1GjRrFx48aowxERSZh424fNAY5IZCCSc/3796dbt27cdtttTJkyJepwREREAPjqq6/o378/1113XdShiIgkTLwVqanAB2Y2zMz6mFnv9CGBsck+mBmjRo3iiCOOoFu3bqxcuTLqkERERDjqqKO46aabGDNmDG+99VbU4YiIJES8FanjgR+BDkAP4JJw6JHXAMxshJktNLN5ZvammVXNpEx5M/vSzL4ys6/N7PaYeQeY2UQzWxL+rZbXmIqS/fffn9dff53ff/+d0aNHRx2OiIgIAMOGDePoo4+mb9++OtEnIsVSXBUpdz85i+GUfIhhItDc3VsCi4EhmZTZCpzi7kcCrYAzzaxdOO8mYLK7Hw5MDsdLlGbNmpGSksKtt94adSgiIiIAlC1blnHjxrF582b69u0bdTgiIvku3u7PMbPqwNnAQe4+wszqAEnuvjwvAbj7RzGjM4ALMinjQPodq2XCwcPxzsBJ4evnCJohDs5LTEVRkyZNAPjhhx9YunQpJ510UrQBiYhIide4cWNGjRpFvXr1og5FRCTfxXVFysw6AIuA7sBt4eTDgSfzOZ7ewPtZxFDKzOYCq4CJ7v5FOKuWu68ECP8emNXKzayfmaWYWcrq1avzN/JCom/fvnTp0oW0tLSoQxEREaFHjx506NABgG3btkUcjYhI/on3HqmHgIvc/UxgRzjtC6BtPAub2SQzW5DJ0DmmzNBw3eMyW4e773T3VkBdoK2ZNY8z9th1jHL3Nu7epmbNmjldvEgYPXo0O3fupGvXrkpYIiJSaNxzzz0cf/zxyk0iUmzEW5Fq4O6Tw9fpTeq2EWfTQHc/1d2bZzK8DWBmPYFzge5hM77s1rWeoPnemeGkX8ysdrie2gRXrEqsQw89lGeffZYvv/ySG264IepwREREgKAJekpKCsOGDYs6FBGRfBFvReobMzsjw7RTgfl5DcDMziS4p6mTu2/OokzN9N78zGy/cNsLw9n/AXqGr3sCb+c1pqKuS5cuXHvttTz66KO88847UYcjIiLCX//6V/r27cu9997Lp59+GnU4IiJ5Fm9nE9cD75jZu8B+ZvY0cB5BRw959RhQDphoZgAz3L1/2JnFGHc/G6gNPGdmpQgqf6+6e3oN4V7gVTPrAywFLsyHmIq8++67jxo1anDKKfnRsaKIiEjejRw5kilTpnDJJZfw1VdfUaVKlahDEhHJNdtHS7o/CwYVmx5AMrAMeDGvPfZFqU2bNp6SkhJ1GAVi48aNJCUlUaFChahDERHZg5nNcvc2UcdRGBXXPDVjxgxOPfVUXn/9dc4888x9LyAiEqHs8lTc3Z+7+wrg/nyLSgrEli1bOOaYYzjmmGMYO3Zs1OGIiEgJ165dO9LS0qhevXrUoYiI5Em890hJEVW+fHnOP/98nn32WZ599tmowxEREdldiXr99ddZvrzINm4RkRJOFakS4Pbbb+fkk0/myiuvZN68eVGHIyIiwi+//EKvXr3o2bMnu3btijocEZEcU0WqBChVqhQvvfQSVatW5YILLuC3336LOiQRESnhatWqxUMPPcTHH3/MQw89FHU4IiI5popUCXHQQQfx8ssvU6NGDX7//feowxEREaFPnz506tSJIUOGMH9+np+oIiJSoLLsbMLMPuXPh+9myd1PzNeIJGE6dOjAZ599RtjNvIiISKTMjDFjxtCiRQv+/ve/M3v2bMqUKRN1WCIiccmu174xMa8PBXoDzwFpQH2Ch9+qG7gixsxYt24dl19+OTfeeCNt27aNOiQRESnBatasyb///W/WrVunSpSIFClZVqTc/bn012Y2AzjD3b+OmfYSQUXqnwmNUPKdu5OSkkLXrl2ZPXs2BxxwQNQhiYhICRb7PKnt27erQiUiRUK890g1Ab7PMO1HoHH+hiMF4YADDuDVV19lxYoV6i1JREQKjZdeeolmzZqxbt26qEMREdmneCtS04B/m9nhZrafmR0BPAN8mrjQJJHatm3Lgw8+yDvvvMOIESOiDkdERITGjRvz448/0r9/f9z3eZu2iEik4q1I9Qr/fg1sAuYDBlyWgJikgAwYMICuXbvy2GOPsWnTpqjDERGREu7oo4/mjjvu4NVXX2XcuHFRhyMikq24KlLuvtbdLwbKA7WB/dy9m7v/mtDoJKHSe0uaOXMmb731Fg0aNCApKYkGDRoogYmISCRuvPFG2rdvz4ABA0hLS4s6HBGRLMX9HCkzawIMBW51911m1sjMWiYuNCkIlSpVYvLkyfTr14+0tDTcnbS0NPr166fKlIiIFLhSpUrx/PPPY2Z8+OGHUYcjIpKluCpSZnYh8AlwMHBpOLkS8GCC4pICNHToUDZv3rzHtM2bNzN06NCIIhIRkZKsYcOGfPfdd/Tr1y/qUEREshTvFak7gNPcvT+wM5z2FXBkQqKSArV06dIcTRcREUm0GjVqAPDpp58yd+7caIMREclEvBWpAwkqTgAe8zfPXeqY2QgzW2hm88zsTTOrmkmZ8mb2pZl9ZWZfm9ntMfOGmdlPZjY3HM7Oa0wlTf369TOdXrFixQKORERE5E9bt26le/fudOvWba+WEyIiUYu3IjULuCTDtIuBL/MhholAc3dvCSwGhmRSZitwirsfCbQCzjSzdjHzR7p7q3B4Lx9iKlGGDx9OhQoV9phWtmxZ/vGPfwCwfv161q5dG0VoIiKFWl5PBkr2ypUrxzPPPMPChQsZPHhw1OGIiOwh3orUNcBdZjYNqGhmHwJ3AtflNQB3/8jdd4SjM4C6mZRxd98YjpYJBz1gIp90796dUaNGkZycjJmRnJzM2LFjue222wAYMmQIRxxxBGPGjNHDe0VE9pQfJwMlG6eddhrXXnstjz32GLVq1VLvsiJSaOyzImVmBmwDmgOPA7cAzwIt3H1JPsfTG3g/izhKmdlcYBUw0d2/iJl9VXg2cKyZVcvnmEqE7t27k5qayq5du0hNTaV79+675/Xv358mTZpw+eWXc9xxxzFr1qwIIxURKTx0MrBgHHnkkZgZq1atUu+yIlJo7LMi5cGjxecDG939VXcf4e4vxySFfTKzSWa2IJOhc0yZocAOINNvRXff6e6tCJJUWzNrHs56EjiU4CzfSuCBbOLoZ2YpZpayevXqeMMv8Y488kg++eQTnn/+eVJTU/nLX/7C6NGjow5LRKSwye3JwNhyylOZGDZsGMHPkT+pd1kRiZpl/GLKtJDZdKCvuy9MSBBmPYH+QEd33+fdpGb2T2CTu/8rw/QGwDvu3jzTBWO0adPGU1JSchlxybVhwwbuuOMOrrnmGpKTk1m3bh1VqlQhKSnuR5KJiOxmZrPcvU3UcWTHzCYBB2Uya6i7vx2WGQq0Abp4Nok1vIfqTeBqd1+Q3XaVp/6UlJS0V0UKggfLq8m5iCRSdnmqdJzrmAp8YGb/BpYR0yTB3cfmMbgzgcFAh6wqUWZWE9ju7uvNbD/gVOC+cF5td18ZFj0fyDYxSd5UqVKFBx4ILvq5OxdffDEbNmzg8ccfp3Xr1hFHJyKS/9z91OzmhycDzyU4GZjt2ckwj00FzkT5Km7169cnLS1tr+lmxrPPPkvPnj11Qk9ECly83zrHAz8CHYAeBD34XRK+zqvHCB7uOzHsvvwpADOrY2bpPfDVBqaY2TxgJkGziHfCefeb2fxw3snkQwcYEr9LLrlkd3O/K664Qr37iUiJEnMysFN2JwPTe/OLORmYkBYexVVmvcuWL1+ehg0b0rt3b92/KyKRiKsi5e4nZzGcktcA3P0wd68X0315/3D6Cnc/O3w9z92PcveW7t7c3e+IWf4Sd28RzusUc3VKEszM6NGjB4sWLWLgwIGMHj2aI444gs8//zzq0ERECkpeTwZKHDLrXXbMmDEsXryYf//736SmpjJ//vyowxSREiaue6T2WCDoxc/Sx929SDZOVtvz/Dd//nxuv/12xo4dS+XKldmyZQvly5ePOiwRKcSKwj1SUVGeit9vv/3G/vvvT1JSEmPGjGHbtm383//9H6VKlYo6NBEp4rLLU3FdkTKzg8MHDa4h6Flve8wgAkCLFi14/fXXqVy5Mtu3b6dt27b079+fNWvWRB2aiIgUY5UrV959j9T777/PgAEDaN26NdOnT484MhEpzuK9R+opgmdJdQQ2AkcD/yHoaU9kL9u3b6djx46MGTOGRo0aMXr0aPWsJCIiCff666/z6quvsnbtWk444QR69OjBihUrog5LRIqheCtSxwG93X0uwaOlvgL6ANcnKjAp2ipUqMDIkSOZM2cOTZs2pV+/frRr146VK3ULm4iIJI6ZceGFF/Ltt98ydOhQXn/9dZYsWRJ1WCJSDMVbkdpJ0KQPYH3YHfkm4OCERCXFRosWLZg2bRovvvgi1atXp2bNmgC6OiUiIglVsWJF7rrrLtLS0ujQoQMADzzwABMnTow4MhEpLuKtSH0BnB2+/hB4BXgD0F2wsk9mRvfu3Xn//fcpXbo069ato1mzZmruJyIiCVerVi0Atm7dypgxYzj99NP529/+RmpqarSBiUiRF29F6hJgWvj6WmAKwYME/56AmKSY++233zjwwAN3N/ebOXNm1CGJiEgxV65cOebMmcPw4cP54IMPaNKkCXfccQd//PFH1KGJSBEV73Ok1rv72vD1H+5+p7sP1jObJDeSk5OZOnUqL774IsuWLeOYY47h//7v/9i+XZ1AiohI4pQvX56bb76ZhQsX0qlTJ4YPH87y5cujDktEiqjS8RQyszuymufut+VfOFJSpDf3O++88xg2bBipqamUKVMm6rBERKQEqFevHq+88gppaWkkJycDMHz4cC644AIaNWoUcXQiUlTE27SvXobhL8ANwKEJiktKiMqVK/Pggw/y+uuvA/Ddd99x4oknqrmfiIgkXHolavny5dx///20aNGCm266iY0bN0YcmYgUBfE27bssw3AW0IU/e/ITyZP0BykuW7aMJUuW7G7up4f5iohIotWtW5fFixfTo0cP7rvvPho1asT48eNx96hDE5FCLN4rUpn5CPhrPsUhAsDJJ5/MokWLuO6663jmmWc44ogjeOaZZ6IOS0REirlatWoxduxYPv/8c2rXrs0111zDb7/9FnVYIlKIxVWRMrNDMgzNgbuAZYkNT0qiypUr88ADDzB37lxatGjBvHnzog5JRERKiHbt2vHFF18wffp0qlSpws6dO7n77rtZt25d1KGJSCET7xWp74Al4d/vgBnACUDPBMUlQvPmzZkyZQr3338/AJ988gn9+vXj119/jTgyEREpzkqVKrW704nPPvuMW2+9dXcLCT3/UETSxXuPVJK7lwr/Jrn7/u5+grvPSnSAUrKZGeXKlQNgzpw5jB07lkaNGvH000+zc+fOiKMTEZHi7sQTT2TWrFk0btyYvn370q5dO7788suowxKRQiAv90iJFKiBAwfubu7Xv39/JTMRESkQrVq14pNPPuHFF19k+fLl9O7dW1emRCTue6SWmdnSfQ25CcDMRpjZQjObZ2ZvmlnVbMqWMrM5ZvZOzLQDzGyimS0J/1bLTRxSNKQ39xs3bhw//fQT//vf/6IOSURESoD05x8uWrSI1157jaSkJH777TeeeuopduxQJ8YiJVG8V6QeBjYAdwJ9w7/rwumXxAy5MRFo7u4tgcXAkGzKDgS+zTDtJmCyux8OTA7HpRgzM/7+97+zcOFCBgwYAMBrr72m5n4iIpJwlSpVokmTJgCMHz+eK664gqOPPppp06ZFHJmIFLR4K1K9gDPdfbS7f+Tuo4FzgMvcfVr6kJsAwvWln8qZAdTNrJyZ1Q23OSbDrM7Ac+Hr51CX7CVG5cqVKVOmDAATJkygf//+HHPMMWruJyIiBaJfv3688cYb/Pbbb5x00kl069aN5cuXRx2WiBSQeCtSdYCMj/neCBycv+HQG3g/i3kPATcCGRsl13L3lQDh3wPzOSYpAsaPH89LL73EihUraNeuHZdffjm//vor48aNo0GDBiQlJdGgQQPGjRsXdagiIlJMmBnnn38+33zzDf/85z958803ueqqq6IOS0QKSOk4y/0H+I+Z3QUsB+oRNMH7TzwLm9kk4KBMZg1197fDMkOBHcBev3TN7FxglbvPMrOT4ow5szj6Af0A6tevn9vVSCFkZnTr1o1zzjmHO+64g4cffphKlSrx9NNPs3nzZgDS0tLo168fAN27d48yXBERKUYqVKjAsGHD6NmzJ+4OBDnn22+/5cwzz4w4OhFJFEv/h8+2kFl5YBhwIcHVqZXAq8Dt7v5HnoMw6wn0Bzq6++ZM5t9DcA/WDqA8UBl4w917mNki4CR3X2lmtYGp7t5oX9ts06aNp6Sk5DV0KaRSU1Pp0KEDS5fu3QdKcnIyqampBR+UiOzFzGa5e5uo4yiMlKeKtoEDB/LII4/QqVMnRo4cySGHHBJ1SCKSC9nlqXifI7XF3W9y90PdfT93PyQcz49K1JnAYKBTZpWocPtD3L2uuzcALgY+dvce4ez/8OeDgXsCb+c1Jin6GjRowLJlyzKdl5aWxqOPPlrAEYmISEkyYsQI7rvvPiZPnkzTpk355z//ubuFhIgUD/F2f36ymTUMXx9kZs+Z2Vgzy6y5Xk49BlQCJprZXDN7KtxOHTN7L47l7wVOM7MlwGnhuEiWzTfLlSu3+2bg7du307BhQzp16sTw4cOZOHEiGzZsKMgwRUSkGCpbtiw33ngjixYt4m9/+xt33HEHt99+e9RhiUg+irdp37fAGe6+1MxeCif/AdR0906JDDBR1GSi+Bs3bhz9+vXb4wxghQoVGDVq1O57pNasWcOgQYP48ssvWbhw4e5yjz/+OFdeeSWbNm1i0aJFtGjRYncPgSKSf9S0L2vKU8XLJ598QtOmTalRowbz58+ndOnSu7tRF5HCK89N+4CDw0pUaeAMgg4brgCOy6cYRfJd9+7dGTVqFMnJyZgZycnJe1SiAKpXr85zzz3Ht99+y7p16/joo4+46667OPbYYwH49NNPad26NZUrV6Z9+/YMGjSIV155hfXr10e0VyIiUhSdeOKJ1KhRA4Drr7+eli1b8o9//IMxY8aod1mRIireK1LLgdZAc2CYu59gZmWB1e5eJcExJoTO9Ek8fv31VyZPnswXX3zBF198wezZs9myZQtz587lyCOPZMqUKfzvf/+jbdu2tG3blipViuS/g0hkdEUqa8pTxdfq1au5+eabGTMm46Mx9245ISLRyi5Pxdv9+aPATKAscG047XhgYVYLiBQHNWrU4KKLLuKiiy4Cgnuq5s2bR7NmzQCYNm3aHm3eGzduzDHHHMOoUaMoW7ZsJDGLiEjhVrNmTUaPHs0777zDzz//vMe8zZs3c/XVV5OcnEyLFi10gk6kEIvrihSAmR0B7HT372PGy7n7/ATGlzA60yf5Zd26daSkpOy+arVixQpmzZoFQK9evfjuu+845phjaNu2Lcccc8zupoYioitS2VGeKv6SkpLY1++w+vXr07JlS5588knq1q3Lxo0bKV++PKVLx3suXETyIj+uSOHui7MbFympqlWrxmmnncZpp52217xGjRqxZMkSnnjiCR588EEATj31VCZOnAjArFmzOOyww3TGUUSkBKpfvz5paWl7TT/44IMZNWoU8+bNY968ecyfP5+qVasCMHz4cEaOHEnTpk1p2bIlLVq0oGXLlnTs2JGkpHhvfReR/BD3FaniRmf6pCClNwn84osv2H///bn00kvZuXMnVapUYfPmzTRu3Hj3FauOHTtyxBFHRB2ySIHQFamsKU8Vf/H0LpvRxx9/zPvvv7+7kvXzzz9TtWpV1q5di5kxYsQIfv75Z1q2bEnLli1p0qQJ5cuXL6hdEil28qPXPhHJgzJlytC6dWuuvPJKLr30UgDcnTfeeIM77riDQw89lPfee48rr7xyd49Nv/32G9dffz2vvvoqaWlpu5t/jBs3Tj08iYgUA/H0LpvRKaecwogRI/jwww9ZuXIlq1atYtKkSbubjH/11Vc88cQT9OrVi6OPPpr999+fzp07717+888/3yOniEju6YqUSCHh7qSlpVG2bFnq1KnDzJkzOfHEE9myZQsAtWrVok6dOnz77be7p4F6eJKiTVeksqY8Jbm1c+dOvvvuu91XrapWrcr1118PBLlk1apVVK5ceXezwLPPPptzzz034qhFCqfs8lSOKlJmdiCwf+w0d/8hb+FFQwlKioLYJoFffPEFL730Ejt27NirXN26dVm2bFkEEYrkjSpSWVOekvzm7syYMWN3BSt96Nu3Lw888ABbtmyhRYsWe91/ddhhh6lzCymx8lyRMrMzgWeAg4DY7sbc3UvlS5QFTAlKiqLseng677zzuPjii7nooosoVapI/ltKCaSKVNaUp6QguDtbt26lfPnyrF69mmuuuYZ58+axaNEidu7cCcADDzzAoEGDWLVqFePGjdt9/1XNmjWzXO+4ceMYOnQoS5cupX79+gwfPlwtJ6RIyo9e+x4H7gSec/c/8i0yEcmRrHp4qlSpEnPmzGH+/Pl069YNCNrJN2rUSDcZi4hIlsxsd56oWbMm48ePB2DLli18++23zJ8/n7Zt2wIwZ84cBg0atHvZWrVq0bJlS+6//35atWrF5s2bSUpKYsKECXt0opGWlka/fv0AVJmSYiXeziaqAU+rEiUSreHDh1OhQoU9plWoUIEnn3yStLQ0pk2bhpmxfft2Tj31VGrVqkWvXr344IMP2L59e0RRi4hIUVO+fHmOOuooLr30Uho3bgzAGWecwS+//MKkSZN48MEHOeuss1i7di3lypUDgqtQ+++/P7169dqjJ0IIHjQ8dOjQAt8PkUSKtyL1DHBZIgMRkX3LroenpKQk6tevDwRNAMeNG8ff/vY33nrrLc466yzq1Kmz+0yjiIhIbhx44IF07NiR6667jmeffZaUlBSaNGkCQOvWrRkyZEim9/ICLF26VL0FSrES7z1SnwJtgTTg59h57n5iYkJLLLU9l5Ji69atfPDBB7z88ssMGDCA9u3bM3v2bF588UW6detGmzZtdnebK1LQdI9U1pSnpKhq0KBBps3Qy5cvT/369enatStdu3alefPmyj9S6OXHc6TGAP2A4QRXp2IHESnEypUrR+fOnRk/fjzt27cHYPbs2Tz++OO0bduWww47jKFDhzJ//vyIIxUpesxshJktNLN5ZvammVXNpmwpM5tjZu8UYIgiBS6rZug9evSgXr163H333bRs2ZKmTZsyevToiKIUybu4KlLu/lxWQ6IDFJH817dvX3755RfGjh3LYYcdxn333Uf79u3ZunUrAL///nvEEYoUGROB5u7eElgMDMmm7EDg2wKJSiRCWTVDHz16NJMmTWLlypU8+eST1K5dm3Xr1gGwadMmbr/9dr755puIoxeJX9zPkTKzWgTN+2oQ0wW6u4/NUwBmI4DzgG3A98Bl7r4+i7KlgBTgJ3c/N5w2DLgcWB0Wu9nd39vXdtVkQuRPq1atYv78+XTs2BF3p2nTplSsWJFu3brRtWtX6tWrF3WIUkwVp6Z9ZnY+cIG779UtmZnVBZ4jaNkxKD2HZUd5SkoCd8fMmDx5MqeddhruTrNmzbjwwgvp2rXr7vuvRKKS56Z9ZvZXgkrOHcDTwNXh30vyIb78OJs30t1bhcM+K1Eisqf0m4cBdu7cyeWXX46ZccMNN1C/fn1OOOEEPvjgg4ijFCn0egPvZzHvIeBGYFd2KzCzfmaWYmYpq1evzq6oSLGQfo9Ux44dWbFiBY899hjVq1fn9ttvp2nTpruvUKW3mBApTOK9R+ougitFRwGbwr/9gFl5DcDdP3L39O5dZgB1MysXns07h+B+LRFJkNKlSzNo0CBmzpzJkiVLuPPOO1m7di0bNmwAYNmyZYwdO5b169dHG6hIATGzSWa2IJOhc0yZocAOYFwmy58LrHL3feZMdx/l7m3cvU12DzsVKY4OOuggBgwYwLRp0/jpp58YO3bs7itS/fv358gjj+Suu+5i8eLFEUcqEoi3IlXf3V/LMO054NJ8jie3Z/OuCm/0HWtm1fI5JpES67DDDuOWW27h66+/5sILLwTg7bffpk+fPtSqVYvOnTvz8ssvs2nTpogjFUkcdz/V3ZtnMrwNYGY9gXOB7p55e/njgU5mlgq8DJxiZi8W2A6IFEG1a9fmsssu233Fqn379lSqVIlbb72VRo0a0apVK5566qmIo5SSLt6K1KrwHimAVDM7FjgUKBXPwgk+m/dkGEsrYCXwQDZxqMmESC4lJQVfFwMGDODLL79kwIABpKSk0K1bN+rUqaPKlJRIZnYmMBjo5O6bMyvj7kPcva67NwAuBj529x4FGKZIkdenTx+mT5/OsmXLeOihh6hYsSKLFi0CgibpDzzwAN99913EUUpJE+9zpAYD37n7BDO7FBhFcGXoAXe/Nc9BBGfz+gMdM0tEZnYPwf1YO4DyQGXgjYyJyMwaAO+4e/N9bVM38Yrk3a5du5g+fTpz587lmmuuAeD888+natWqdOvWjVNOOYXSpUtHHKUUZkW9swkz+w4oB6wJJ81w9/5mVgcY4+5nZyh/EnCDOpsQybtdu3aRlJTE7Nmzad26NQBHHXUUXbt25cILL+TQQw+NOEIpDrLLU3H32pdhhfWBiu6e525cw7N5DwId3H2fl4kyJiEzq+3uK8PX1wHHuPvF+1qPEpRI/tu1axd9+/ZlwoQJ/Pbbb9SsWZMLL7yQvn37ctRRR0UdnhRCRb0ilUjKUyLxW7ZsGa+//jqvvvoqM2bMAODTTz+lffv27Ny5k1Kl4mpEJbKX/HggL2ZWxsxOMLOL3H0psNTMKuZDfI8BlYCJZjbXzJ4Kt1fHzOLpge9+M5tvZvOAk4Hr8iEmEcmFpKQkxo4dyy+//MIbb7zBySefzLPPPsu0adOA4PlUKSkp5OYEjoiISFbq1avHddddx+eff05qaioPPvggxxxzDAC33HILf/nLXxgxYgSpqanRBirFSrzdn7cg6Jp8NPBMOLkDkKdnSAG4+2HuXi+m+/L+4fQVGZtEhNOnxjaJcPdL3L2Fu7d0907pV6dEJDrly5fn/PPP55VXXmHVqlX06dMHgDfeeIO//OUvHHHEEdx66618/fXXu5cZN24cDRo0ICkpiQYNGjBu3F63S4qIiOxTcnIy1113HWXKlAGgUaNGmBk33ngjDRs2pG3btjz++OMRRynFQbxXpJ4EbnP3xsD2cNo0oH1CohKRYmP//fenUqVKAHTq1IkxY8bQoEED7r77bpo3b07Lli0ZM2YM/fr1Iy0tDXcnLS2Nfv36qTIlIiJ51qtXL7788kt++OEH7r//ftydTz/9dPf8sWPHsnTp0ggjlKIq3s4m1gEHuLub2Vp3PyCcvvt1UaO25yLR+vnnn3n99deZNWsWU6ZMIS0tba8yycnJaoZRzOkeqawpT4kkzrZt2yhbtiypqak0bNgQgHbt2tG1a1cuuOAC6tWrF3GEUljkxz1SqUDrDCttC6ifSRHJlYMOOoirrrqKZ599NsszgTpDKCIiiVC2bFkAGjRowJIlS7j77rvZunUrgwYNon79+rzzzjsAe9zTqyboklG8FalbgXfN7HagrJkNAV4DbklYZCJSYtSvXz/T6eXLl2f58uUFHI2IiJQkhx12GEOGDGH27NksXryY4cOH0759cPfKo48+Svv27bnkkku4/PLL1QRd9hBXRcrd3wHOAmoS3BuVDHRx948SGJuIlBDDhw+nQoUKe0wrU6YMSUlJbNu2LaKoRESkpDn88MO5+eabqVq1KgAHHHAAv//+Oy+++CJ//PHHHmU3b97M0KFDI4hSCou4uz9399nufqW7n+Pu/d19ViIDE5GSo3v37owaNYrk5GTMjOTkZJ599ll++eUXDjnkENydq666ig8++CDqUEVEpATp0aMHX331FWaW6Xw1QS/Z4u3+vLSZXWJmD5rZqNgh0QGKSMnQvXt3UlNT2bVrF6mpqXTv3p2KFYNH1a1du5aJEydy1lln0aVLl0w7phAREUmUrJqgly1blilTphRwNFJYxHtF6kXgJmAX8EuGQUQkoapXr868efO45557+PDDD2nSpAl33XUXW7ZsiTo0EREpATJrgl6uXDkqVqzIKaecwnnnnce3334bUXQSlXgrUmcC7dz9Bne/NXZIZHAiIunKlSvHTTfdxMKFCznnnHN49NFHVZESEZECkVkT9GeeeYbly5dz77338sknn9CiRQsGDBhAPI8WkuIh3orUN0CRfF6UiBQv9erV47XXXmP+/PlUrVqVHTt2cN111+l5UyIiklCZNUHfb7/9GDx4MN999x1XXnkl27dv330/1fbt2yOOWBIt3opUD2CMmf3DzC6NHRIZnIhIVg488EAAvvrqK0aPHk2TJk248847dZVKREQKXM2aNXnkkUd4+umnAZg1axaHHnoozz33HLt27Yo4OkmUeCtSvYATgIuAy2OGvokJS0QkPq1bt2bhwoV06tSJ2267jebNm/Puu+9GHZaIiJRA6VejzIyDDjqIXr160bp1ayZPnhxxZJII8VakBgJHuXsbdz8hZjgxkcGJiMSjbt26vPLKK0ycOJEyZcpw0003sXPnzqjDEhGREuroo49mxowZjB8/nnXr1nHqqafStWtX3T9VzMRbkfoFUEf5IlKonXrqqXz11Vf897//pVSpUmzYsIF77713r4coioiIJFpSUhIXX3wxCxcuZMSIEbRr1w4zw91ZvXp11OFJPoi3IjUSGGdm7czskNghkcGJiORU2bJladCgAQBvvfUWQ4YMoVmzZvz3v/+NNjARESmRypcvzw033MCgQYMAePPNN2nYsCF33nknmzZtijg6yYt4K1KPA52A/wHfxQxLEhSXiEie9ezZk0mTJlG+fHk6derEeeedxw8//BB1WCIiUoIdeeSRnHXWWdx2220cccQRjB07Vs3Ri6i4KlLunpTFUCqvAZjZCDNbaGbzzOxNM6uaRblUM5tvZnPNLCVm+gFmNtHMloR/q+U1JhEpPjp27MjcuXMZMWIEU6dO3X1GUEREJAqHHnoor732Gp999hn169enT58+dO7cOeqwJBfivSKVSBOB5u7eElgMDMmm7Mnu3srd28RMuwmY7O6HA5PDcRGR3cqWLcsNN9zAwoULeeSRRwD48ccf+c9//qMbf0VEJBLHHXcc//vf/3jttde4/PLLAdi6dSsLFiyIODKJV+QVKXf/yN13hKMzgLo5XEVn4Lnw9XPAX/MpNBEpZg4++GDq168PwMiRI+ncuTPnnnsu3333XcSRiYhISWRmXHDBBbuvSD355JMceeSR9O3blxUrVkQcnexL5BWpDHoD72cxz4GPzGyWmfWLmV7L3VcChH8PTHCMIlIMPPDAAzzwwAN88sknNGvWjFtvvZXNmzdHHZaIiJRgPXv25Nprr+X555/n8MMPZ9iwYWzcuDHqsCQLBVKRMrNJZrYgk6FzTJmhwA5gXBarOd7djwbOAgaYWY6fYWVm/cwsxcxS1O2kSMlWpkwZBg0axKJFi7jgggu46667GD58eNRhiYhICVatWjUeeOABFi5cyLnnnsvtt99Ot27dog5LsmCF4f4AM+sJ9Ac6uvs+Twmb2TBgo7v/y8wWASe5+0ozqw1MdfdG+1pHmzZtPCUlZV/FRKSEmDZtGi1btqRatWrMmzeP/fbbj8MPPzzqsIo9M5uV4b5XCSlPiciMGTMoU6YMrVu3ZvXq1aSkpHDmmWdiZlGHVmJkl6fydEXKzN7Ny/LhOs4EBgOdsqpEmVlFM6uU/ho4HUi/E+8/QM/wdU/g7bzGJCIlT4cOHahWLej085prrqF58+bccsstau4nIiKRadeuHa1btwbg0Ucf5eyzz+b0009n7ty50QYmQN6b9k3PhxgeAyoBE8OuzZ8CMLM6ZvZeWKYWMN3MvgK+BN519w/CefcCp5nZEuC0cFxEJNfGjx9P165dGT58OE2aNOGNN95Q734iIhKpW265hYcffpjZs2dz9NFHc9lll7F8+fKowyrRCkXTviioyYSI7Munn37KgAEDmD9/Pi+//DIXXXRR1CEVO2ralzXlKRHJzPr167n77rt5+OGHueCCCxg3LqvuBSQ/ZJenSse5gkOymLUVWOnuu3IbnIhIYXXCCScwe/Zsnn/+ebp06QLA3LlzOfzww6lYsWLE0YmISElUtWpV7r//fq644gpKlw5+yn/zzTd8+umn9OnTZ/c0Sbx4m/Z9BywJh9jXS4GtZjbBzGolJkQRkeiULl2a3r17U6ZMGbZu3co555xDkyZNmDBhgpr7iYhIZBo2bEi9evUAeOGFF+jfvz8tW7bk3XffVX4qIPFWpC4n6Jb8CKA80Ah4EbgSaEFwZevxRAQoIlJYlCtXjldeeYVq1apxwQUXcMYZZ7Bo0aKowxIRkRLu7rvv5s0332THjh2ce+65nHrqqcyZMyfqsIq9eCtStwP93P17d9/m7t8BVwC3uvtCoBdwUmJCFBEpPNq3b8+sWbN45JFH+OKLL2jRogVff/111GGJiEgJZmb89a9/5euvv+bRRx/lq6++4uWXX446rGIv3opUEtAgw7T6QKnw9UbivN9KRKSoK126NFdffTWLFy/m7rvvpmnTpgAsWrRIzSlERCQyZcqU4aqrruL777/nlltuAWDSpEncfPPN/PbbbxFHV/zEW5F6CPjYzIabWX8zuwuYHE4HOAf4PP/DExEpvGrVqsUNN9yAmbFs2TKOOuooTj/9dBYuXBh1aCIiUoJVqVKFSpUqATB9+nTuueceDjvsMJ544gm2b98ecXTFR1wVKXe/H+gNHAR0BuoAfdz9vnD+W+5+VsKiFBEp5GrXrs3999/PzJkzadmyJYMHD2bjxo2MGzeOBg0akJSURIMGDdRNrYiIFKhhw4Yxc+ZMmjZtyoABA2jRogUffvhh1GEVC3E/kNfdP3D3Pu5+lrv3jnkgrohIiVe6dGmuuuoqFi9eTI8ePbj//vtJTk7m8ssvJy0tDXcnLS2Nfv36qTIlIiIFqk2bNkyZMoW3334bgB9//HH3PJ3wy724HshrZmWAW4BLCK5GrQBeAIa7+7aERpggetChiCTS559/ztlnn8369ev3mpecnExqamqBx1QY6YG8WVOeEpFE2L59O2ZG6dKl6dOnD8899xw7d+7cPb9ChQqMGjWK7t27Rxhl4ZFdnor3itT9wKlAf+DI8O8pwH35EqGISDFz7LHHsmHDhkznLV26tICjERERCZQpU2b3Q3vfeOONPSpRAJs3b2bo0KFRhFbkxNvT3oXAke6+JhxfZGazga+A6xISmYhIEVe/fn3S0tL2ml69enXcHTOLICoREZGATvjlTbxXpLLK9voVICKSheHDh1OhQoU9ppkZv/76K126dFFX6SIiEqn69etnOV05at/irUi9BvzXzM4wsyZmdibwFvBqwiITESniunfvzqhRo0hOTsbMSE5O5rnnnuPRRx+lY8eOmBnuzh9//BF1qJJLZjbCzBaa2Twze9PMqmZRLtXM5pvZXDPTjU8iUihkdsKvQoUKDB8+nFtuuYUBAwawdu3aiKIr/OKtSN0ITAIeB2YBjwFTgMEJiktEpFjo3r07qamp7Nq1i9TUVC655BKuuuoqrrrqKgBee+01GjduzJtvvqmzf0XTRKC5u7cEFgNDsil7sru3UucaIlJYZHbCL72jiW3btvHUU09xxBFH8PTTT+91L5XE/xypbe5+m7sf5u4Vwr+3uvvWRAcoIlKc1atXj6pVq9KlSxfOPvtslixZEnVIkgPu/pG77whHZwB1o4xHRCSnMp7wS++tb8SIEcydO5fmzZvTv39/2rZty9y5c6MNtpDJsiJlZqfEMxRksCIixc2xxx7LrFmzeOihh/jss89o3rw5Dz30UNRhSe70Bt7PYp4DH5nZLDPrl9UKzKyfmaWYWcrq1asTEqSISLxatGjBlClTePnll1m7di1lypSJOqRCJcvnSJnZj5nO2JO7+yF5CsBsBHAesA34HrjM3ddnUi4V+B3YCexIbxphZsOAy4H0jHOzu7+3r+3q+RwiUtisXLmSG2+8kfPOO4+uXbuya9cuzKxY9+5XFJ4jZWaTgIMymTXU3d8OywwF2gBdPJPEamZ13H2FmR1I0Bzwanf/JLvtKk+JSGGyY8eO3d2mX3755TRq1IhrrrmGsmXLRhxZYmWXp7Ls/tzdGyYupD1MBIa4+w4zu4+gfXlW916d7O6/ZjJ9pLv/K2ERiogUgNq1a/PCCy/sHh8xYgSffvopjzzyCIcckqdzVpIH7n5qdvPNrCdwLtAxs0pUuI4V4d9VZvYm0BbItiIlIlKYpFeitm3bxsqVKxkzZgxjxozhkUce4fTTT484umjE29lEwqh9uYhI5ipVqsS0adNo2rQpt99+u3r3K4TCXmwHA53cfXMWZSqaWaX018DpwIKCi1JEJP+ULVuWd955h3feeYedO3dyxhlncP7557NixYqoQytwkVekMsht+/Krwq5nx5pZtcSGKCJSMK688koWLlzI+eefz7Bhw2jevDmffKKLGIXMY0AlYGLYtflTEDTlM7P0Zua1gOlm9hXwJfCuu38QTbgiIvnjnHPOYcGCBdx9993MnDlz9xWrkqRAKlJmNsnMFmQydI4pMxTYAYzLYjXHu/vRwFnAADM7MZz+JHAo0ApYCTyQTRy6iVdEipSDDz6Y8ePHM2nSJMqXL1/s26IXNWEvtvXCbs1buXv/cPoKdz87fP2Dux8ZDs3cfXi0UYuI5I9y5coxZMgQvv/+ew488EB27dpF165dmTBhQol4pEeBVKTc/VR3b57JkH6Tbnr78u7xtC8H0tuX4+6/uPtOd98FjE6fnsU6Rrl7G3dvU7NmzfzdSRGRBOrYsSPz58+nXbt2AAwaNIi77rqLrVv1FAoREYlWuXLlAFi1ahULFy7kggsu4LTTTuObb76JOLLEirxpX17bl5tZ7Zii56N25yJSTCUlBV/Zu3bt4qeffuLWW2+lefPmfPjhhxFHJiIiAgcddBCzZ8/m0UcfZdasWRx55JEMGjSITZs2RR1aQkRekSLv7cvvN7P5ZjYPOBm4roDjFxEpUElJSbzyyit8+OGHJCUlceaZZ/K3v/2Nn376KerQRESkhCtdujRXXXUVixcvplevXrzzzjuUKlUq6rASIvK7wtz9sCymrwB2ty8Hjsyi3CWJi05EpPA6/fTTmTdvHg8++CAPPvggO3bs2PdCIiIiBaBmzZqMHj2azZs3U758eTZv3kzPnj256aabaN26ddTh5YvCcEVKRERyKf1G37S0NJKTk3F3rrzySiZNmhR1aCIiIlSoUAGARYsW8emnn/KXv/yFfv368euvmT0atmhRRUpEpBhIT1SrV6/mo48+4rTTTqNr164sX7484shERETgqKOOYtGiRVx77bWMHTuWww8/nMcee4xdu3ZFHVquqSIlIlKMHHjggSxYsIA777yT//73vzRu3JgRI0awbdu2qEMTEZESrkqVKjz44IPMmzeP1q1b8+qrr2JmUYeVa6pIiYgUM+XLl+eWW27hm2++oWPHjowcOZItW7ZEHZaIiAgATZs2ZeLEifz3v//FzFixYgWXXXZZkes0SRUpEZFiqmHDhrz99tvMmTOHypUrs337dm688UZWrFgRdWgiIlLCmRlVqlQBYMaMGYwfP55GjRpx7733FplnJKoiJSJSzNWqVQuAmTNn8sgjj9CoUSMefPBBtm/fHnFkIiIi0KVLF7755htOPfVUhgwZQosWLXjvvff2vWDEVJESESkhjjvuOBYsWMAJJ5zA9ddfz9FHH80nn3wSdVgiIiIccsghvPXWW3zwwQeYGa+88krUIe2TKlIiIiXIYYcdxrvvvstbb73F77//zsCBA4t0j0kiIlK8nHHGGcyfP5+HH34YgLlz5zJ06FA2bdoUcWR7U0VKRKSEMTM6d+7MN998w4QJE0hKSmLdunU8/vjjeqiviIhErmzZslStWhWADz/8kLvvvpvGjRvzyiuv4O7RBhdDFSkRkRKqQoUKHHLIIQCMGzeOq666itatWzN9+vSIIxMREQkMHjyY6dOnU6NGDS6++GJOOeUU5s+fH3VYgCpSIiICDBgwgAkTJrBu3TpOOOEEevXqxS+//BJ1WCIiIhx//PGkpKTw5JNPMm/ePF5//fWoQwJUkRIREYLmfl26dOHbb79lyJAhvPTSS1x77bVRhyUiIgJAqVKl6N+/P0uWLGHw4MEAfPDBB4wZMyaye31VkRIRkd0qVqzI3Xffzfz587n33nsB+P777/n8888jjkxERAQOOOAAKlSoAATN0i+//HLatWvHF198UeCxqCIlIiJ7adSoEcnJyQDceeedHHfccfTp04fVq1dHHJmIiEjg+eef54UXXmD58uW0a9eO3r17F2izdFWkREQkW4899hg33ngjzz//PI0aNeKpp57ihRdeoEGDBiQlJdGgQQPGjRsXdZgiIlLCmBk9evRg0aJF/OMf/+DFF1/kgw8+AIKrVYnOU1aYuhAsSG3atPGUlJSowxARKTK++eYbBgwYwNSpUylTpgzbt2/fPa9ChQqMGjWK7t2752idZjbL3dvkd6zFgfKUiEjOfP/99zRs2JDx48fTu3dvtm3btnteIvJU5FekzGyEmS00s3lm9qaZVc2iXFUzez0s+62ZHRtOP8DMJprZkvBvtQLdARGREqJp06Z8/PHH1KhRY49KFMDmzZsZOnRoRJGJiIjAoYceSlJSEjfffPMelShITJ6KvCIFTASau3tLYDEwJItyDwMfuHtj4Ejg23D6TcBkdz8cmByOi4hIApgZa9asyXTe0qVLCzgaERGRvS1btizT6fmdpyKvSLn7R+6+IxydAdTNWMbMKgMnAs+Ey2xz9/Xh7M7Ac+Hr54C/JjJeEZGSrn79+jmaLiIiUpAKKk9FXpHKoDfwfibTDwFWA8+a2RwzG2NmFcN5tdx9JUD498CsVm5m/cwsxcxS1POUiEjuDB8+fHfXs+kqVKjA8OHDI4pIRETkTwWVpwqkImVmk8xsQSZD55gyQ4EdQGZdapQGjgaedPejgE3kogmfu49y9zbu3qZmzZq53BsRkZKte/fujBo1iuTkZMyM5OTkXN3AKyIikggFlacKRa99ZtYT6A90dPfNmcw/CJjh7g3C8ROAm9z9HDNbBJzk7ivNrDYw1d0b7Wub6g1JRCR66rUva8pTIiLRK+y99p0JDAY6ZVaJAnD3n4FlZpZeQeoIfBO+/g/QM3zdE3g7geGKiIiIiIhEX5ECHgMqARPNbK6ZPQVgZnXM7L2YclcD48xsHtAKuDucfi9wmpktAU4Lx0VERERERBKmdNQBuPthWUxfAZwdMz4X2OuymruvIbhCJSIiIiIiUiAKwxUpERERERGRIkUVKRERERERkRwqFL32RcHMVgNpeVhFDeDXfAqnMNF+FR3FcZ9A+1XU5HW/kt1dz6PIhPJUlrRfRUtx3K/iuE+g/cpKlnmqxFak8srMUopjl73ar6KjOO4TaL+KmuK6X8VBcT022q+ipTjuV3HcJ9B+5Yaa9omIiIiIiOSQKlIiIiIiIiI5pIpU7o2KOoAE0X4VHcVxn0D7VdQU1/0qDorrsdF+FS3Fcb+K4z6B9ivHdI+UiIiIiIhIDumKlIiIiIiISA6pIiUiIiIiIpJDqkiJiIiIiIjkUImuSJnZAWb2ppltMrM0M/t7FuXKmdlIM1thZuvM7AkzKxMzv4mZfWxmG8zsOzM7P8PyfcPpG83sAzOrk+h9i9l2gexjQcvH/ZpqZlvCY7PRzBYV3F7sW37tZ9RysB9PxRyLjWa21cx+j5lfaI9XDvaxl5ntzLCfJxVstFnLwX70NLNZZvabmS03s/vNrHTM/EJ7rIoS5ak9yilPFcL/JeUp5amCVqjylLuX2AEYD7wC7A+0BzYAzTIp90/gU+AAoCYwA7g9nFcaWAwMAkoBpwCbgCPC+R2AVUAzoCzwJDCtOO1jUT124fypQN+oP4uJ3s+oh3j3I5Pl/g2MLQrHKwfHqhcwPep482E/rgBOCL/XDgZmATcVhWNVlAblqfzZx6J67ML5hfp/SXlKeaoQ70fC81Tkb0aEB6EisC32SxZ4Abg3k7IpwIUx438HloWvmwMbCXtADKd9BNwZvv4X8HjMvDqAA4cWl30sqscuHC/MX3j5tp9FZT8yWe53oENhP145PFaFNkHl9liF5QYB/y3sx6ooDcpT+bePRfXYheOF9n9JeUp5qjDvRybL5nueKslN+44Adrr74phpXxGckcvIwiF2vK6ZVckwPXZ+82yWJWZ+IhXUPha0/NqvdPeY2a9m9llhunRN/u9nVHKyH7H+BqwGPskwvTAer5zu41HhPiw2s1tjmxpELLfHCuBE4OsM0wrjsSpKlKf2pDxV+P6XlKeUpwpaocpTJbkitT/BpcBYG4BKmZR9HxhoZjXN7CDgmnB6BWAhQZOIf5hZGTM7naCZRIWwzHtAVzNraWb7AbcRnOmrQOIV1D4WtPzaL4DBwCEEl3xHAf81s0PzP+Rcyc/9jFJO9iNWT+B5D08bhQrr8crJPn5C8OPuQIIk3A34R0Kji1+ujpWZXQa0Ibiyka6wHquiRHlqT8pThe9/SXlKeaqgFao8VZIrUhuByhmmVSa4RJvRcGAOMBf4H/AWsB1Y5e7bgb8C5wA/A9cDrwLLAdx9MkG74AlAGpAabmN5/u1KlgpkHyOQL/sF4O5fuPvv7r7V3Z8DPgPOTkzYOZZv+xmxnOwHAGZWj+BH0POx0wvx8Yp7H939B3f/0d13uft84A7gggKIMR65OVZ/Be4FznL3X9OnF+JjVZQoT+1Jearw/S8pTylPFbRCladKckVqMVDazA6PmXYke1/yw93/cPer3P1gdz8EWAPMcved4fx57t7B3au7+xkEtdsvY5Z/3N0Pd/cDCRJVaWBB4nZttwLbxwKWb/uVCSfzJiJRSOR+FqS49yPGpcD/3P2Hfay7sByv3OxjusKyD5DD/TCzM4HRwHlhss1OYdrPokJ5Koby1J+LUHj+l5SnlKcKWuHKU14IbhyLagBeJuj5oyJwPFn3+nEwwc23BrQDlgGnx8xvCZQnuDx9A/AjUC6cV57g8qgB9QlubLu7OO1jUT12QFXgjHC/SgPdCXp5ahT1ZzO/j1/UQ7z7EVN+EdA7w7RCfbxycKzOAmqFrxsT/Fj9Z9Tx52I/TiH4IXRiJvMK9bEqSkNBfIejPFVo96so/C/l1/GLeoh3P2LKK08V/v1IeJ6K/M2I+EAcQHBpeROwFPh7OL0+waXD+uH4iQRNHTaH/zjdM6xnBLAuXOZ94LAMB2peuI2fgXuAUsVpH4vqsSPofnUmweXg9QRdsZ4W9ecyEccv6iHe/QinHRuWq5RhHYX6eOXgWP0L+CUs9wNBk4kyUcefi/2YAuwIp6UP7xeFY1WUhvz6DkB5qkgeu6Lwv5Rfxy/qId79CKcpTxWN/Uh4nrJwZSIiIiIiIhKnknyPlIiIiIiISK6oIiUiIiIiIpJDqkiJiIiIiIjkkCpSIiIiIiIiOaSKlIiIiIiISA6pIiUiIiIiIpJDqkiJ5AMzO9/MlpnZRjM7Ko/rOsHMFuVh+fphHKXyEkd+MzM3s8OijkNEpCRSnto35SnJKT1HSkoEM+sF9HX39gla//fAIHd/OxHrLw7MzIHD3f27qGMRESlslKeipzwlOaUrUlLkmVnpqGMAkoGvow5CREQKH+UpkeJJFSkpksws1cwGm9k8YJOZlTazm8zsezP73cy+MbPzw7JNgKeAY8OmBOvD6eXM7F9mttTMfjGzp8xsvyy2l2Rmt5hZmpmtMrPnzaxKuI6NQCngq/CMX2bLu5ldaWZLwvjuNLNDzexzM/vNzF41s7Jh2ZPMbHnMsoPN7KdwuUVm1jGc3tbMUsLlfzGzB8PpDcLtlQ7Hp4bb+yxcx0dmViNm/ZeG+7XGzG4N39tTM9mHdmb2c2xTjLCpyLyYeD43s/VmttLMHkvfp0zWNdXM+saM9zKz6THjjc1sopmtDfe5a8y8s8Pj+3v4vtyQ2TZERKKkPKU8pTxV/KkiJUVZN+AcoKq77wC+B04AqgC3Ay+aWW13/xboD3zu7vu7e9Vw+fuAI4BWwGHAwcBtWWyrVzicDBwC7A885u5b3X3/sMyR7n5oNvGeCbQG2gE3AqOA7kA9oHm4P3sws0bAVcBf3L0ScAaQGs5+GHjY3SsDhwKvZrPtvwOXAQcCZYEbwvU3BZ4I46hN8N4dnNkK3H0GsAk4JcN6Xwpf7wSuA2oAxwIdgSuziSlTZlYRmBiu90CC9+UJM2sWFnkG+L/w/WgOfJzTbYiIFBDlKeUp5aliTBUpKcoecfdl7v4HgLu/5u4r3H2Xu78CLAHaZragmRlwOXCdu69199+Bu4GLs9hWd+BBd//B3TcCQ4CLLWfNNe5z99/c/WtgAfBRuL4NwPtAZjf/7gTKAU3NrIy7p7p7+tnE7cBhZlbD3TeGCSQrz7r74vC9epUgKQNcAPzX3ae7+zaCBJ3djZPjCROpmVUCzg6n4e6z3H2Gu+9w91TgaaBD9m9Jps4FUt392XBds4EJYawQ7HdTM6vs7uvC+SIihZHylPKU8lQxpoqUFGXLYkfCS/9zw0v26wnOAtXIdEmoCVQAZsWU/yCcnpk6QFrMeBpQGqiVg3h/iXn9Rybj+5NBeMPrtcAwYJWZvWxmdcLZfQjOVC40s5lmdm422/455vXmmG3VIeZ9dPfNwJps1vMS0MXMygFdgNnungZgZkeY2Tths4rfCBJ+Vu9/dpKBY9KPS3hsugMHhfP/RpAY08xsmpkdm4ttiIgUBOUp5SnlqWJMFSkpynafkTKzZGA0QfOC6mGziAWAZSwb+pUgKTRz96rhUCWm+UNGKwi+ONPVB3awZ5JJCHd/KezFKZlgP+4Lpy9x924EzQruA14PmxvkxEqgbvqIBW3vq2cTyzcEyfks9mwuAfAksJCgx6PKwM38+f5ntIngB0K6g2JeLwOmxRyXqmFTlyvCGGa6e2eC/X6L7JuKiIhESXlKeeotlKeKLVWkpLioSPDlvRrAzC4jONOX7hegbvpNpe6+iyChjTSzA8NlDjazM7JY/3jgOjNraGb7E5zFeiVs854wZtbIzE4Jz6xtIUiqO8N5PcysZrgv68NFduZwE68D55nZceF7cztZJ5V0LwHXACcCr8VMrwT8Bmw0s8bAFdmsYy7BGcMKFjyzo0/MvHeAI8zsEjMrEw5/MbMmZlbWzLqbWRV33x5uL6f7LCISBeWpgPKUFBuqSEmxEJ6BegD4nCAZtQA+iynyMUG3rz+b2a/htMHAd8CM8BL/JKBRFpsYC7wAfAL8SJAsrs7n3chMOeBegjOTPxOc3bo5nHcm8LUFvTE9DFzs7ltysvKwHfzVwMsEZ/1+B1YBW7NZbDxwEvCxu/8aM/0GgrN/vxMk/1eyWcdIYBvBsXoOGBcT0+/A6QT3Aawg2O/7CN4LgEuA1PCY9Qd67HtPRUSipTylPCXFjx7IKyK7hWcx1xM0e/gx4nBERET2oDwlhYmuSImUcGZ2Xth0oSLwL2A+f3ZdKyIiEinlKSmsVJESkc4ETRNWAIcTNL3QpWoRESkslKekUFLTPhERERERkRzSFSkREREREZEcUkVKREREREQkh1SREhERERERySFVpERERERERHJIFSkREREREZEc+n8A6tYQFRWukgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1008x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set colors for individual experiments:\n",
    "colors = ['black', 'blue', 'red', 'cyan', 'magenta', 'yellow']\n",
    "linestyles = ['--', '--', '--', '--', '--', '--']\n",
    "\n",
    "# Plot final train and validation loss over sparsity for all experiments:\n",
    "fig, axes = plt.subplots(1,2,figsize=(14,4))\n",
    "\n",
    "# exp1:\n",
    "axes[0].plot(\n",
    "        np.log10(train_loss_min_mean_exp1), color=colors[0],\n",
    "        label = 'mask: fixed',\n",
    "        marker = 'o', linestyle = linestyles[0],\n",
    "    )\n",
    "\n",
    "axes[1].plot(\n",
    "        np.log10(val_loss_min_mean_exp1), color=colors[0],\n",
    "        label = 'mask: fixed',\n",
    "        marker = 'o', linestyle = linestyles[0],\n",
    "    )\n",
    "\n",
    "# # exp2:\n",
    "# axes[0].plot(\n",
    "#         np.log10(train_loss_min_mean_exp2), color=colors[1],\n",
    "#         label = 'mask: variable / discrete, augmentation factor: 1',\n",
    "#         marker = 'o', linestyle = linestyles[1],\n",
    "#     )\n",
    "\n",
    "# axes[1].plot(\n",
    "#         np.log10(val_loss_min_mean_exp2), color=colors[1],\n",
    "#         label = 'mask: variable / discrete, augmentation factor: 1',\n",
    "#         marker = 'o', linestyle = linestyles[1],\n",
    "#     )\n",
    "\n",
    "# # exp3a:\n",
    "# axes[0].plot(\n",
    "#         np.log10(train_loss_min_mean_exp3a), color=colors[2],\n",
    "#         label = 'mask: variable / discrete, augmentation factor: 2',\n",
    "#         marker = 'o', linestyle = linestyles[2],\n",
    "#     )\n",
    "\n",
    "# axes[1].plot(\n",
    "#         np.log10(val_loss_min_mean_exp3a), color=colors[2],\n",
    "#         label = 'mask: variable / discrete, augmentation factor: 2',\n",
    "#         marker = 'o', linestyle = linestyles[2],\n",
    "#     )\n",
    "\n",
    "# # exp3b:\n",
    "# axes[0].plot(\n",
    "#         np.log10(train_loss_min_mean_exp3b), color=colors[3],\n",
    "#         label = 'mask: variable / discrete, augmentation factor: 3',\n",
    "#         marker = 'o', linestyle = linestyles[3],\n",
    "#     )\n",
    "\n",
    "# axes[1].plot(\n",
    "#         np.log10(val_loss_min_mean_exp3b), color=colors[3],\n",
    "#         label = 'mask: variable / discrete, augmentation factor: 3',\n",
    "#         marker = 'o', linestyle = linestyles[3],\n",
    "#     )\n",
    "\n",
    "# # exp4a:\n",
    "# axes[0].plot(\n",
    "#         np.log10(train_loss_min_mean_exp4a), color=colors[4],\n",
    "#         label = 'mask: optimal from CESM',\n",
    "#         marker = 'o', linestyle = linestyles[4],\n",
    "#     )\n",
    "\n",
    "# axes[1].plot(\n",
    "#         np.log10(val_loss_min_mean_exp4a), color=colors[4],\n",
    "#         label = 'mask: optimal from CESM',\n",
    "#         marker = 'o', linestyle = linestyles[4],\n",
    "#     )\n",
    "\n",
    "# # exp4b:\n",
    "# axes[0].plot(\n",
    "#         np.log10(train_loss_min_mean_exp4b), color=colors[5],\n",
    "#         label = 'mask: optimal from FOCI',\n",
    "#         marker = 'o', linestyle = linestyles[5],\n",
    "#     )\n",
    "\n",
    "# axes[1].plot(\n",
    "#         np.log10(val_loss_min_mean_exp4b), color=colors[5],\n",
    "#         label = 'mask: optimal from FOCI',\n",
    "#         marker = 'o', linestyle = linestyles[5],\n",
    "#     )\n",
    "\n",
    "# #plt.ylim(0.6, 0.8)\n",
    "axes[0].set_xticks(np.arange(0, len(missing_values), step=1), missing_values, fontsize=12)\n",
    "axes[1].set_xticks(np.arange(0, len(missing_values), step=1), missing_values, fontsize=12)\n",
    "axes[0].set_title('SLP real world: Mean training loss', fontsize=16)\n",
    "axes[1].set_title('SLP real world: Mean validation loss', fontsize=16)\n",
    "axes[0].set_xlabel('rate of missing values', fontsize=12)\n",
    "axes[1].set_xlabel('rate of missing values', fontsize=12)\n",
    "axes[0].set_ylabel('log. mean squared error', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b1019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7801a45a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

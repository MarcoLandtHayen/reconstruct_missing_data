{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58748dd7",
   "metadata": {},
   "source": [
    "### Evaluate final U-Net models on CESM and FOCI slp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a53d0aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../reconstruct_missing_data')\n",
    "\n",
    "from pathlib import Path\n",
    "from json import dump, load\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from data_loading import (\n",
    "    find_data_files, \n",
    "    load_data_set, \n",
    "    get_anomalies, \n",
    "    clone_data, \n",
    "    create_missing_mask, \n",
    "    split_and_scale_data,\n",
    "    area_mean_weighted,\n",
    "    spatial_mask,\n",
    ")\n",
    "from models import build_unet_4conv\n",
    "from indices import (\n",
    "    southern_annular_mode_zonal_mean,\n",
    "    north_atlantic_oscillation_station,\n",
    "    north_pacific,\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, concatenate, Conv1D, Conv2D, MaxPool2D, UpSampling2D, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow.keras.initializers as tfi\n",
    "import tensorflow.keras.regularizers as tfr\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Suppress Tensorflow warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e2fb9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory, according to working directory in scripts:\n",
    "os.chdir('/gxfs_work1/geomar/smomw511')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff41b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set paths to final models:\n",
    "\n",
    "# CESM\n",
    "paths_to_final_models=[\n",
    "    'GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_CESM_fixed_discrete_factor_1_final',\n",
    "    'GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_CESM_variable_discrete_factor_1_final',\n",
    "    'GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_CESM_variable_discrete_factor_2_final',\n",
    "    'GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_CESM_variable_discrete_factor_3_final',\n",
    "    'GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_CESM_optimal_discrete_factor_1_final',\n",
    "]\n",
    "\n",
    "# FOCI\n",
    "# paths_to_final_models=[\n",
    "#     'GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_FOCI_fixed_discrete_factor_1_final',\n",
    "#     'GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_FOCI_variable_discrete_factor_1_final',\n",
    "#     'GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_FOCI_variable_discrete_factor_2_final',\n",
    "#     'GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_FOCI_variable_discrete_factor_3_final',\n",
    "#     'GitGeomar/marco-landt-hayen/reconstruct_missing_data_results/unet_4conv_slp_FOCI_optimal_discrete_factor_1_final',\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23fa61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and validation loss per sample for first model, to infer number of training and validation samples, \n",
    "# lat and lon, and number of missing value rates. Do this from first experiment, since it has augmentation factor 1:\n",
    "n_train=np.load(Path(paths_to_final_models[0]) / 'train_loss_per_sample_all.npy').shape[-1]\n",
    "n_val=np.load(Path(paths_to_final_models[0]) / 'val_loss_per_sample_all.npy').shape[-1]\n",
    "lat=np.load(Path(paths_to_final_models[0]) / 'train_loss_map_all.npy').shape[1]\n",
    "lon=np.load(Path(paths_to_final_models[0]) / 'train_loss_map_all.npy').shape[2]\n",
    "n_missing=np.load(Path(paths_to_final_models[0]) / 'train_loss_per_sample_all.npy').shape[0]\n",
    "\n",
    "# Initialize storage for parameters:\n",
    "feature_all = []\n",
    "feature_short_all = []\n",
    "source_all = []\n",
    "mask_type_all = []\n",
    "missing_type_all = []\n",
    "augmentation_factor_all = []\n",
    "missing_values_all = []\n",
    "\n",
    "# Initialize storage for reloaded results, for all models and missing rates.\n",
    "train_loss_per_sample_all = np.zeros((len(paths_to_final_models),n_missing,n_train))\n",
    "val_loss_per_sample_all = np.zeros((len(paths_to_final_models),n_missing,n_val))\n",
    "train_loss_map_all = np.zeros((len(paths_to_final_models),n_missing,lat,lon))\n",
    "val_loss_map_all = np.zeros((len(paths_to_final_models),n_missing,lat,lon))\n",
    "SAM_train_pred_all = np.zeros((len(paths_to_final_models),n_missing,n_train))\n",
    "SAM_val_pred_all = np.zeros((len(paths_to_final_models),n_missing,n_val))\n",
    "SAM_train_target_all = np.zeros((len(paths_to_final_models),n_missing,n_train))\n",
    "SAM_val_target_all = np.zeros((len(paths_to_final_models),n_missing,n_val))\n",
    "NAO_train_pred_all = np.zeros((len(paths_to_final_models),n_missing,n_train))\n",
    "NAO_val_pred_all = np.zeros((len(paths_to_final_models),n_missing,n_val))\n",
    "NAO_train_target_all = np.zeros((len(paths_to_final_models),n_missing,n_train))\n",
    "NAO_val_target_all = np.zeros((len(paths_to_final_models),n_missing,n_val))\n",
    "NP_train_pred_all = np.zeros((len(paths_to_final_models),n_missing,n_train))\n",
    "NP_val_pred_all = np.zeros((len(paths_to_final_models),n_missing,n_val))\n",
    "NP_train_target_all = np.zeros((len(paths_to_final_models),n_missing,n_train))\n",
    "NP_val_target_all = np.zeros((len(paths_to_final_models),n_missing,n_val))\n",
    "\n",
    "# Loop over final models:\n",
    "for i in range(len(paths_to_final_models)):\n",
    "    \n",
    "    # Get path to final model:\n",
    "    path_to_final_model = paths_to_final_models[i]\n",
    "    \n",
    "    # Reload parameters for this experiment:\n",
    "    with open(Path(path_to_final_model) / 'parameters.json', 'r') as f:\n",
    "        parameters=load(f)\n",
    "\n",
    "    # Store parameters:    \n",
    "    feature_all.append(parameters['feature'])\n",
    "    feature_short_all.append(parameters['feature_short'])\n",
    "    source_all.append(parameters['source'])\n",
    "    mask_type_all.append(parameters['mask_type'])\n",
    "    missing_type_all.append(parameters['missing_type'])\n",
    "    augmentation_factor_all.append(parameters['augmentation_factor'])\n",
    "    missing_values_all.append(parameters['missing_values'])\n",
    "    \n",
    "    ## Reload results.\n",
    "    ## Take augmentation factor into account: If data is used n times, only store every n-th sample.\n",
    "    ## Like this, we have equal dimensions for all augmentation factors and include each target only once.\n",
    "    \n",
    "    # Get step size from augmentation factor:\n",
    "    step = augmentation_factor_all[-1]\n",
    "\n",
    "    # Get number of missing value rates:\n",
    "    n_missing = len(missing_values_all[-1])\n",
    "\n",
    "    ## Reload results and filter:\n",
    "    # Consider all missing value rates, although optimal model has only 99.9%, 99% and 95% missing values. \n",
    "    # And take each sample only once.\n",
    "    train_loss_per_sample_all[i,:n_missing]=np.load(Path(path_to_final_model) / 'train_loss_per_sample_all.npy')[:,np.arange(0,n_train*step,step)]\n",
    "    val_loss_per_sample_all[i,:n_missing]=np.load(Path(path_to_final_model) / 'val_loss_per_sample_all.npy')[:,np.arange(0,n_val*step,step)]\n",
    "    train_loss_map_all[i,:n_missing]=np.load(Path(path_to_final_model) / 'train_loss_map_all.npy')\n",
    "    val_loss_map_all[i,:n_missing]=np.load(Path(path_to_final_model) / 'val_loss_map_all.npy')\n",
    "    SAM_train_pred_all[i,:n_missing]=np.load(Path(path_to_final_model) / 'SAM_train_pred_all.npy')[:,np.arange(0,n_train*step,step)]\n",
    "    SAM_val_pred_all[i,:n_missing]=np.load(Path(path_to_final_model) / 'SAM_val_pred_all.npy')[:,np.arange(0,n_val*step,step)]\n",
    "    SAM_train_target_all[i,:n_missing]=np.load(Path(path_to_final_model) / 'SAM_train_target_all.npy')[:,np.arange(0,n_train*step,step)]\n",
    "    SAM_val_target_all[i,:n_missing]=np.load(Path(path_to_final_model) / 'SAM_val_target_all.npy')[:,np.arange(0,n_val*step,step)]\n",
    "    NAO_train_pred_all[i,:n_missing]=np.load(Path(path_to_final_model) / 'NAO_train_pred_all.npy')[:,np.arange(0,n_train*step,step)]\n",
    "    NAO_val_pred_all[i,:n_missing]=np.load(Path(path_to_final_model) / 'NAO_val_pred_all.npy')[:,np.arange(0,n_val*step,step)]\n",
    "    NAO_train_target_all[i,:n_missing]=np.load(Path(path_to_final_model) / 'NAO_train_target_all.npy')[:,np.arange(0,n_train*step,step)]\n",
    "    NAO_val_target_all[i,:n_missing]=np.load(Path(path_to_final_model) / 'NAO_val_target_all.npy')[:,np.arange(0,n_val*step,step)]\n",
    "    NP_train_pred_all[i,:n_missing]=np.load(Path(path_to_final_model) / 'NP_train_pred_all.npy')[:,np.arange(0,n_train*step,step)]\n",
    "    NP_val_pred_all[i,:n_missing]=np.load(Path(path_to_final_model) / 'NP_val_pred_all.npy')[:,np.arange(0,n_val*step,step)]\n",
    "    NP_train_target_all[i,:n_missing]=np.load(Path(path_to_final_model) / 'NP_train_target_all.npy')[:,np.arange(0,n_train*step,step)]\n",
    "    NP_val_target_all[i,:n_missing]=np.load(Path(path_to_final_model) / 'NP_val_target_all.npy')[:,np.arange(0,n_val*step,step)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8cce9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
